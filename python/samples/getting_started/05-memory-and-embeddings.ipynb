{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {},
   "source": [
    "# Building Semantic Memory with Embeddings\n",
    "\n",
    "So far, we've mostly been treating the kernel as a stateless orchestration engine.\n",
    "We send text into a model API and receive text out.\n",
    "\n",
    "In a [previous notebook](04-kernel-arguments-chat.ipynb), we used `kernel arguments` to pass in additional\n",
    "text into prompts to enrich them with more data. This allowed us to create a basic chat experience.\n",
    "\n",
    "However, if you solely relied on kernel arguments, you would quickly realize that eventually your prompt\n",
    "would grow so large that you would run into the model's token limit. What we need is a way to persist state\n",
    "and build both short-term and long-term memory to empower even more intelligent applications.\n",
    "\n",
    "To do this, we dive into the key concept of `Semantic Memory` in the Semantic Kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713abcd",
   "metadata": {},
   "source": [
    "Import Semantic Kernel SDK from pypi.org and other dependencies for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.22.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: if using a virtual environment, do not run this cell\n",
    "# %pip install -U semantic-kernel[azure]\n",
    "from semantic_kernel import __version__\n",
    "\n",
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318033fe",
   "metadata": {},
   "source": [
    "Initial configuration for the notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a3db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f9651",
   "metadata": {},
   "source": [
    "### Configuring the Kernel\n",
    "\n",
    "Let's get started with the necessary configuration to run Semantic Kernel. For Notebooks, we require a `.env` file with the proper settings for the model you use. Create a new file named `.env` and place it in this directory. Copy the contents of the `.env.example` file from this directory and paste it into the `.env` file that you just created.\n",
    "\n",
    "**NOTE: Please make sure to include `GLOBAL_LLM_SERVICE` set to either OpenAI, AzureOpenAI, or HuggingFace in your .env file. If this setting is not included, the Service will default to AzureOpenAI.**\n",
    "\n",
    "#### Option 1: using OpenAI\n",
    "\n",
    "Add your [OpenAI Key](https://openai.com/product/) key to your `.env` file (org Id only if you have multiple orgs):\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_ORG_ID=\"\"\n",
    "OPENAI_CHAT_MODEL_ID=\"\"\n",
    "OPENAI_TEXT_MODEL_ID=\"\"\n",
    "OPENAI_EMBEDDING_MODEL_ID=\"\"\n",
    "```\n",
    "The names should match the names used in the `.env` file, as shown above.\n",
    "\n",
    "#### Option 2: using Azure OpenAI\n",
    "\n",
    "Add your [Azure Open AI Service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=programming-language-studio) settings to the `.env` file in the same folder:\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"AzureOpenAI\"\n",
    "AZURE_OPENAI_API_KEY=\"...\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://...\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_TEXT_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_API_VERSION=\"...\"\n",
    "```\n",
    "The names should match the names used in the `.env` file, as shown above.\n",
    "\n",
    "For more advanced configuration, please follow the steps outlined in the [setup guide](./CONFIGURING_THE_KERNEL.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815cac6e",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b95af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.OpenAI\n"
     ]
    }
   ],
   "source": [
    "from services import Service\n",
    "\n",
    "from samples.service_settings import ServiceSettings\n",
    "\n",
    "service_settings = ServiceSettings()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "In order to use memory, we need to instantiate the Kernel with a Memory Storage\n",
    "and an Embedding service. In this example, we make use of the `VolatileMemoryStore` which can be thought of as a temporary in-memory storage. This memory is not written to disk and is only available during the app session.\n",
    "\n",
    "When developing your app you will have the option to plug in persistent storage like Azure AI Search, Azure Cosmos Db, PostgreSQL, SQLite, etc. Semantic Memory allows also to index external data sources, without duplicating all the information as you will see further down in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    "    OpenAIChatCompletion,\n",
    "    OpenAITextEmbedding,\n",
    ")\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if selectedService == Service.OpenAI:\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        ai_model_id=\"gpt-4o-mini\",        # ← chat model ID\n",
    "        service_id=\"chat\"\n",
    "    )\n",
    "    embedding_gen = OpenAITextEmbedding(\n",
    "        ai_model_id=\"text-embedding-3-small\",  # ← embedding model ID (required)\n",
    "        service_id=\"embedding\"\n",
    "    )\n",
    "    kernel.add_service(oai_chat_service)\n",
    "    kernel.add_service(embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a612a26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel.connectors.ai.embedding_generator_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, ValidationError, model_validator\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass \u001b[38;5;28;01mas\u001b[39;00m pyd_dataclass\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_generator_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingGeneratorBase\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptExecutionSettings\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shared\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     DEFAULT_FUNCTION_NAME,\n\u001b[1;32m     24\u001b[0m     DEFAULT_PARAMETER_METADATA,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     default_dynamic_filter_function,\n\u001b[1;32m     31\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'semantic_kernel.connectors.ai.embedding_generator_base'"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import operator\n",
    "import sys\n",
    "from abc import abstractmethod\n",
    "from ast import AST, Lambda, NodeVisitor, expr, parse\n",
    "from collections.abc import AsyncIterable, Callable, Mapping, Sequence\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from inspect import Parameter, _empty, getsource, signature\n",
    "from types import MappingProxyType, NoneType\n",
    "from typing import Annotated, Any, ClassVar, Final, Generic, Literal, Protocol, TypeVar, overload, runtime_checkable\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, model_validator\n",
    "from pydantic.dataclasses import dataclass as pyd_dataclass\n",
    "\n",
    "from semantic_kernel.connectors.ai.embedding_generator_base import EmbeddingGeneratorBase\n",
    "from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n",
    "from semantic_kernel.data._shared import (\n",
    "    DEFAULT_FUNCTION_NAME,\n",
    "    DEFAULT_PARAMETER_METADATA,\n",
    "    DEFAULT_RETURN_PARAMETER_METADATA,\n",
    "    DynamicFilterFunction,\n",
    "    KernelSearchResults,\n",
    "    SearchOptions,\n",
    "    create_options,\n",
    "    default_dynamic_filter_function,\n",
    ")\n",
    "from semantic_kernel.exceptions import (\n",
    "    VectorSearchExecutionException,\n",
    "    VectorSearchOptionsException,\n",
    "    VectorStoreModelDeserializationException,\n",
    "    VectorStoreModelException,\n",
    "    VectorStoreModelSerializationException,\n",
    "    VectorStoreModelValidationError,\n",
    "    VectorStoreOperationException,\n",
    "    VectorStoreOperationNotSupportedException,\n",
    ")\n",
    "from semantic_kernel.exceptions.search_exceptions import TextSearchException\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.functions.kernel_function import KernelFunction\n",
    "from semantic_kernel.functions.kernel_function_from_method import KernelFunctionFromMethod\n",
    "from semantic_kernel.functions.kernel_parameter_metadata import KernelParameterMetadata\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseModel\n",
    "from semantic_kernel.kernel_types import OneOrList, OneOrMany, OptionalOneOrList, OptionalOneOrMany\n",
    "from semantic_kernel.utils.feature_stage_decorator import release_candidate\n",
    "from semantic_kernel.utils.list_handler import desync_list\n",
    "\n",
    "if sys.version_info >= (3, 11):\n",
    "    from typing import Self  # pragma: no cover\n",
    "else:\n",
    "    from typing_extensions import Self  # pragma: no cover\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "TModel = TypeVar(\"TModel\", bound=object)\n",
    "TKey = TypeVar(\"TKey\")\n",
    "_T = TypeVar(\"_T\", bound=\"VectorStoreRecordHandler\")\n",
    "TFilters = TypeVar(\"TFilters\")\n",
    "\n",
    "DEFAULT_DESCRIPTION: Final[str] = (\n",
    "    \"Perform a vector search for data in a vector store, using the provided search options.\"\n",
    ")\n",
    "\n",
    "\n",
    "# region: Fields and Collection Definitions\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class FieldTypes(str, Enum):\n",
    "    \"\"\"Enumeration for field types in vector store models.\"\"\"\n",
    "\n",
    "    KEY = \"key\"\n",
    "    VECTOR = \"vector\"\n",
    "    DATA = \"data\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return the string representation of the enum.\"\"\"\n",
    "        return self.value\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class SerializeMethodProtocol(Protocol):\n",
    "    \"\"\"Data model serialization protocol.\n",
    "\n",
    "    This can optionally be implemented to allow single step serialization and deserialization\n",
    "    for using your data model with a specific datastore.\n",
    "    \"\"\"\n",
    "\n",
    "    def serialize(self, **kwargs: Any) -> Any:\n",
    "        \"\"\"Serialize the object to the format required by the data store.\"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class ToDictFunctionProtocol(Protocol):\n",
    "    \"\"\"Protocol for to_dict function.\n",
    "\n",
    "    Args:\n",
    "        record: The record to be serialized.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, record: Any, **kwargs: Any) -> Sequence[dict[str, Any]]: ...  # pragma: no cover\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class FromDictFunctionProtocol(Protocol):\n",
    "    \"\"\"Protocol for from_dict function.\n",
    "\n",
    "    Args:\n",
    "        records: A list of dictionaries.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        A record or list thereof.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, records: Sequence[dict[str, Any]], **kwargs: Any) -> Any: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class SerializeFunctionProtocol(Protocol):\n",
    "    \"\"\"Protocol for serialize function.\n",
    "\n",
    "    Args:\n",
    "        record: The record to be serialized.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        The serialized record, ready to be consumed by the specific store.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, record: Any, **kwargs: Any) -> Any: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class DeserializeFunctionProtocol(Protocol):\n",
    "    \"\"\"Protocol for deserialize function.\n",
    "\n",
    "    Args:\n",
    "        records: The serialized record directly from the store.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        The deserialized record in the format expected by the application.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, records: Any, **kwargs: Any) -> Any: ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class ToDictMethodProtocol(Protocol):\n",
    "    \"\"\"Class used internally to check if a model has a to_dict method.\"\"\"\n",
    "\n",
    "    def to_dict(self, *args: Any, **kwargs: Any) -> dict[str, Any]:\n",
    "        \"\"\"Serialize the object to the format required by the data store.\"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "\n",
    "class IndexKind(str, Enum):\n",
    "    \"\"\"Index kinds for similarity search.\n",
    "\n",
    "    HNSW\n",
    "        Hierarchical Navigable Small World which performs an approximate nearest neighbor (ANN) search.\n",
    "        Lower accuracy than exhaustive k nearest neighbor, but faster and more efficient.\n",
    "\n",
    "    Flat\n",
    "        Does a brute force search to find the nearest neighbors.\n",
    "        Calculates the distances between all pairs of data points, so has a linear time complexity,\n",
    "        that grows directly proportional to the number of points.\n",
    "        Also referred to as exhaustive k nearest neighbor in some databases.\n",
    "        High recall accuracy, but slower and more expensive than HNSW.\n",
    "        Better with smaller datasets.\n",
    "\n",
    "    IVF Flat\n",
    "        Inverted File with Flat Compression.\n",
    "        Designed to enhance search efficiency by narrowing the search area\n",
    "        through the use of neighbor partitions or clusters.\n",
    "        Also referred to as approximate nearest neighbor (ANN) search.\n",
    "\n",
    "    Disk ANN\n",
    "        Disk-based Approximate Nearest Neighbor algorithm designed for efficiently searching\n",
    "        for approximate nearest neighbors (ANN) in high-dimensional spaces.\n",
    "        The primary focus of DiskANN is to handle large-scale datasets that cannot fit entirely\n",
    "        into memory, leveraging disk storage to store the data while maintaining fast search times.\n",
    "\n",
    "    Quantized Flat\n",
    "        Index that compresses vectors using DiskANN-based quantization methods for better efficiency in the kNN search.\n",
    "\n",
    "    Dynamic\n",
    "        Dynamic index allows to automatically switch from FLAT to HNSW indexes.\n",
    "\n",
    "    Default\n",
    "        Default index type.\n",
    "        Used when no index type is specified.\n",
    "        Will differ per vector store.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    HNSW = \"hnsw\"\n",
    "    FLAT = \"flat\"\n",
    "    IVF_FLAT = \"ivf_flat\"\n",
    "    DISK_ANN = \"disk_ann\"\n",
    "    QUANTIZED_FLAT = \"quantized_flat\"\n",
    "    DYNAMIC = \"dynamic\"\n",
    "    DEFAULT = \"default\"\n",
    "\n",
    "\n",
    "class DistanceFunction(str, Enum):\n",
    "    \"\"\"Distance functions for similarity search.\n",
    "\n",
    "    Cosine Similarity\n",
    "        the cosine (angular) similarity between two vectors\n",
    "        measures only the angle between the two vectors, without taking into account the length of the vectors\n",
    "        Cosine Similarity = 1 - Cosine Distance\n",
    "        -1 means vectors are opposite\n",
    "        0 means vectors are orthogonal\n",
    "        1 means vectors are identical\n",
    "    Cosine Distance\n",
    "        the cosine (angular) distance between two vectors\n",
    "        measures only the angle between the two vectors, without taking into account the length of the vectors\n",
    "        Cosine Distance = 1 - Cosine Similarity\n",
    "        2 means vectors are opposite\n",
    "        1 means vectors are orthogonal\n",
    "        0 means vectors are identical\n",
    "    Dot Product\n",
    "        measures both the length and angle between two vectors\n",
    "        same as cosine similarity if the vectors are the same length, but more performant\n",
    "    Euclidean Distance\n",
    "        measures the Euclidean distance between two vectors\n",
    "        also known as l2-norm\n",
    "    Euclidean Squared Distance\n",
    "        measures the Euclidean squared distance between two vectors\n",
    "        also known as l2-squared\n",
    "    Manhattan\n",
    "        measures the Manhattan distance between two vectors\n",
    "    Hamming\n",
    "        number of differences between vectors at each dimensions\n",
    "    DEFAULT\n",
    "        default distance function\n",
    "        used when no distance function is specified\n",
    "        will differ per vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    COSINE_SIMILARITY = \"cosine_similarity\"\n",
    "    COSINE_DISTANCE = \"cosine_distance\"\n",
    "    DOT_PROD = \"dot_prod\"\n",
    "    EUCLIDEAN_DISTANCE = \"euclidean_distance\"\n",
    "    EUCLIDEAN_SQUARED_DISTANCE = \"euclidean_squared_distance\"\n",
    "    MANHATTAN = \"manhattan\"\n",
    "    HAMMING = \"hamming\"\n",
    "    DEFAULT = \"DEFAULT\"\n",
    "\n",
    "\n",
    "DISTANCE_FUNCTION_DIRECTION_HELPER: Final[dict[DistanceFunction, Callable[[int | float, int | float], bool]]] = {\n",
    "    DistanceFunction.COSINE_SIMILARITY: operator.gt,\n",
    "    DistanceFunction.COSINE_DISTANCE: operator.le,\n",
    "    DistanceFunction.DOT_PROD: operator.gt,\n",
    "    DistanceFunction.EUCLIDEAN_DISTANCE: operator.le,\n",
    "    DistanceFunction.EUCLIDEAN_SQUARED_DISTANCE: operator.le,\n",
    "    DistanceFunction.MANHATTAN: operator.le,\n",
    "    DistanceFunction.HAMMING: operator.le,\n",
    "}\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "@dataclass\n",
    "class VectorStoreField:\n",
    "    \"\"\"Vector store fields.\"\"\"\n",
    "\n",
    "    field_type: Literal[FieldTypes.DATA, FieldTypes.KEY, FieldTypes.VECTOR] = FieldTypes.DATA\n",
    "    name: str = \"\"\n",
    "    storage_name: str | None = None\n",
    "    type_: str | None = None\n",
    "    # data specific fields (all optional)\n",
    "    is_indexed: bool | None = None\n",
    "    is_full_text_indexed: bool | None = None\n",
    "    # vector specific fields (dimensions is mandatory)\n",
    "    dimensions: int | None = None\n",
    "    embedding_generator: EmbeddingGeneratorBase | None = None\n",
    "    # defaults for these fields are not set here, because they are not relevant for data and key types\n",
    "    index_kind: IndexKind | None = None\n",
    "    distance_function: DistanceFunction | None = None\n",
    "\n",
    "    @overload\n",
    "    def __init__(\n",
    "        self,\n",
    "        field_type: Literal[FieldTypes.KEY, \"key\"] = FieldTypes.KEY,  # type: ignore[assignment]\n",
    "        *,\n",
    "        name: str | None = None,\n",
    "        type: str | None = None,\n",
    "        storage_name: str | None = None,\n",
    "    ):\n",
    "        \"\"\"Key field of the record.\n",
    "\n",
    "        When the key will be auto-generated by the store, make sure it has a default, usually None.\n",
    "\n",
    "        Args:\n",
    "            field_type: always \"key\".\n",
    "            name: The name of the field.\n",
    "            storage_name: The name of the field in the store, uses the field name by default.\n",
    "            type: The type of the field.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(\n",
    "        self,\n",
    "        field_type: Literal[FieldTypes.DATA, \"data\"] = FieldTypes.DATA,  # type: ignore[assignment]\n",
    "        *,\n",
    "        name: str | None = None,\n",
    "        type: str | None = None,\n",
    "        storage_name: str | None = None,\n",
    "        is_indexed: bool | None = None,\n",
    "        is_full_text_indexed: bool | None = None,\n",
    "    ):\n",
    "        \"\"\"Data field in the record.\n",
    "\n",
    "        Args:\n",
    "            field_type: always \"data\".\n",
    "            name: The name of the field.\n",
    "            storage_name: The name of the field in the store, uses the field name by default.\n",
    "            type: The type of the field.\n",
    "            is_indexed: Whether the field is indexed.\n",
    "            is_full_text_indexed: Whether the field is full text indexed.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(\n",
    "        self,\n",
    "        field_type: Literal[FieldTypes.VECTOR, \"vector\"] = FieldTypes.VECTOR,  # type: ignore[assignment]\n",
    "        *,\n",
    "        name: str | None = None,\n",
    "        type: str | None = None,\n",
    "        dimensions: Annotated[int, Field(gt=0)],\n",
    "        storage_name: str | None = None,\n",
    "        index_kind: IndexKind | None = None,\n",
    "        distance_function: DistanceFunction | None = None,\n",
    "        embedding_generator: EmbeddingGeneratorBase | None = None,\n",
    "    ):\n",
    "        \"\"\"Vector field in the record.\n",
    "\n",
    "        This field should contain the value you want to use for the vector.\n",
    "        When passing in the embedding generator, the embedding will be\n",
    "        generated locally before upserting.\n",
    "        If this is not set, the store should support generating the embedding for you.\n",
    "        If you want to retrieve the original content of the vector,\n",
    "        make sure to set this field twice,\n",
    "        once with the VectorStoreRecordDataField and once with the VectorStoreRecordVectorField.\n",
    "\n",
    "        If you want to be able to get the vectors back, make sure the type allows this, especially for pydantic models.\n",
    "        For instance, if the input is a string, then the type annotation should be `str | list[float] | None`.\n",
    "\n",
    "        If you want to cast the vector that is returned, you need to set the deserialize_function,\n",
    "        for instance: `deserialize_function=np.array`, (with `import numpy as np` at the top of your file).\n",
    "        If you want to set it up with more specific options, use a lambda, a custom function or a partial.\n",
    "\n",
    "        Args:\n",
    "            field_type: always \"vector\".\n",
    "            name: The name of the field.\n",
    "            storage_name: The name of the field in the store, uses the field name by default.\n",
    "            type: Property type.\n",
    "                For vectors this should be the inner type of the vector.\n",
    "                By default the vector will be a list of numbers.\n",
    "                If you want to use a numpy array or some other optimized format,\n",
    "                set the cast_function with a function\n",
    "                that takes a list of floats and returns a numpy array.\n",
    "            dimensions: The number of dimensions of the vector, mandatory.\n",
    "            index_kind: The index kind to use, uses a default index kind when None.\n",
    "            distance_function: The distance function to use, uses a default distance function when None.\n",
    "            embedding_generator: The embedding generator to use.\n",
    "                If this is set, the embedding will be generated locally before upserting.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        field_type=FieldTypes.DATA,\n",
    "        *,\n",
    "        name=None,\n",
    "        type=None,\n",
    "        storage_name=None,\n",
    "        is_indexed=None,\n",
    "        is_full_text_indexed=None,\n",
    "        dimensions=None,\n",
    "        index_kind=None,\n",
    "        distance_function=None,\n",
    "        embedding_generator=None,\n",
    "    ):\n",
    "        \"\"\"Vector store field.\"\"\"\n",
    "        self.field_type = field_type if isinstance(field_type, FieldTypes) else FieldTypes(field_type)\n",
    "        # when a field is created, the name can be empty,\n",
    "        # when a field get's added to a definition, the name needs to be there.\n",
    "        if name:\n",
    "            self.name = name\n",
    "        self.storage_name = storage_name\n",
    "        self.type_ = type\n",
    "        self.is_indexed = is_indexed\n",
    "        self.is_full_text_indexed = is_full_text_indexed\n",
    "        if field_type == FieldTypes.VECTOR:\n",
    "            if dimensions is None:\n",
    "                raise ValidationError(\"Vector fields must specify 'dimensions'\")\n",
    "            self.dimensions = dimensions\n",
    "            self.index_kind = index_kind or IndexKind.DEFAULT\n",
    "            self.distance_function = distance_function or DistanceFunction.DEFAULT\n",
    "            self.embedding_generator = embedding_generator\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorStoreCollectionDefinition(KernelBaseModel):\n",
    "    \"\"\"Collection definition for vector stores.\n",
    "\n",
    "    Args:\n",
    "        fields: The fields of the record.\n",
    "        container_mode: Whether the record is in container mode.\n",
    "        to_dict: The to_dict function, should take a record and return a list of dicts.\n",
    "        from_dict: The from_dict function, should take a list of dicts and return a record.\n",
    "        deserialize: The deserialize function, should take a type specific to a datastore and return a record.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fields: list[VectorStoreField]\n",
    "    key_name: str = Field(default=\"\", init=False)\n",
    "    container_mode: bool = False\n",
    "    collection_name: str | None = None\n",
    "    to_dict: ToDictFunctionProtocol | None = None\n",
    "    from_dict: FromDictFunctionProtocol | None = None\n",
    "    serialize: SerializeFunctionProtocol | None = None\n",
    "    deserialize: DeserializeFunctionProtocol | None = None\n",
    "\n",
    "    @property\n",
    "    def names(self) -> list[str]:\n",
    "        \"\"\"Get the names of the fields.\"\"\"\n",
    "        return [field.name for field in self.fields]\n",
    "\n",
    "    @property\n",
    "    def storage_names(self) -> list[str]:\n",
    "        \"\"\"Get the names of the fields for storage.\"\"\"\n",
    "        return [field.storage_name or field.name for field in self.fields]\n",
    "\n",
    "    @property\n",
    "    def key_field(self) -> VectorStoreField:\n",
    "        \"\"\"Get the key field.\"\"\"\n",
    "        return next((field for field in self.fields if field.name == self.key_name), None)  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def key_field_storage_name(self) -> str:\n",
    "        \"\"\"Get the key field storage name.\"\"\"\n",
    "        return self.key_field.storage_name or self.key_field.name\n",
    "\n",
    "    @property\n",
    "    def vector_fields(self) -> list[VectorStoreField]:\n",
    "        \"\"\"Get the names of the vector fields.\"\"\"\n",
    "        return [field for field in self.fields if field.field_type == FieldTypes.VECTOR]\n",
    "\n",
    "    @property\n",
    "    def data_fields(self) -> list[VectorStoreField]:\n",
    "        \"\"\"Get the names of the data fields.\"\"\"\n",
    "        return [field for field in self.fields if field.field_type == FieldTypes.DATA]\n",
    "\n",
    "    @property\n",
    "    def vector_field_names(self) -> list[str]:\n",
    "        \"\"\"Get the names of the vector fields.\"\"\"\n",
    "        return [field.name for field in self.fields if field.field_type == FieldTypes.VECTOR]\n",
    "\n",
    "    @property\n",
    "    def data_field_names(self) -> list[str]:\n",
    "        \"\"\"Get the names of all the data fields.\"\"\"\n",
    "        return [field.name for field in self.fields if field.field_type == FieldTypes.DATA]\n",
    "\n",
    "    def try_get_vector_field(self, field_name: str | None = None) -> VectorStoreField | None:\n",
    "        \"\"\"Try to get the vector field.\n",
    "\n",
    "        If the field_name is None, then the first vector field is returned.\n",
    "        If no vector fields are present None is returned.\n",
    "\n",
    "        Args:\n",
    "            field_name: The field name.\n",
    "\n",
    "        Returns:\n",
    "            VectorStoreRecordVectorField | None: The vector field or None.\n",
    "        \"\"\"\n",
    "        if field_name is None:\n",
    "            if len(self.vector_fields) == 0:\n",
    "                return None\n",
    "            return self.vector_fields[0]\n",
    "        for field in self.fields:\n",
    "            if field.name == field_name or field.storage_name == field_name:\n",
    "                if field.field_type == FieldTypes.VECTOR:\n",
    "                    return field\n",
    "                raise VectorStoreModelException(\n",
    "                    f\"Field {field_name} is not a vector field, it is of type {type(field).__name__}.\"\n",
    "                )\n",
    "        raise VectorStoreModelException(f\"Field {field_name} not found.\")\n",
    "\n",
    "    def get_storage_names(self, include_vector_fields: bool = True, include_key_field: bool = True) -> list[str]:\n",
    "        \"\"\"Get the names of the fields for the storage.\n",
    "\n",
    "        Args:\n",
    "            include_vector_fields: Whether to include vector fields.\n",
    "            include_key_field: Whether to include the key field.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The names of the fields.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            field.storage_name or field.name\n",
    "            for field in self.fields\n",
    "            if field.field_type == FieldTypes.DATA\n",
    "            or (field.field_type == FieldTypes.VECTOR and include_vector_fields)\n",
    "            or (field.field_type == FieldTypes.KEY and include_key_field)\n",
    "        ]\n",
    "\n",
    "    def get_names(self, include_vector_fields: bool = True, include_key_field: bool = True) -> list[str]:\n",
    "        \"\"\"Get the names of the fields.\n",
    "\n",
    "        Args:\n",
    "            include_vector_fields: Whether to include vector fields.\n",
    "            include_key_field: Whether to include the key field.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The names of the fields.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            field.name\n",
    "            for field in self.fields\n",
    "            if field.field_type == FieldTypes.DATA\n",
    "            or (field.field_type == FieldTypes.VECTOR and include_vector_fields)\n",
    "            or (field.field_type == FieldTypes.KEY and include_key_field)\n",
    "        ]\n",
    "\n",
    "    def model_post_init(self, _: Any):\n",
    "        \"\"\"Validate the fields.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelException: If there is a field with an embedding property name\n",
    "                but no corresponding vector field.\n",
    "            VectorStoreModelException: If there is no key field.\n",
    "        \"\"\"\n",
    "        if len(self.fields) == 0:\n",
    "            raise VectorStoreModelException(\n",
    "                \"There must be at least one field with a VectorStoreRecordField annotation.\"\n",
    "            )\n",
    "        for field in self.fields:\n",
    "            if not field.name or field.name == \"\":\n",
    "                raise VectorStoreModelException(\"Field names must not be empty.\")\n",
    "            if field.field_type == FieldTypes.KEY:\n",
    "                if self.key_name != \"\":\n",
    "                    raise VectorStoreModelException(\"Memory record definition must have exactly one key field.\")\n",
    "                self.key_name = field.name\n",
    "        if not self.key_name:\n",
    "            raise VectorStoreModelException(\"Memory record definition must have exactly one key field.\")\n",
    "\n",
    "\n",
    "# region: Decorator\n",
    "\n",
    "\n",
    "def _parse_vector_store_record_field_instance(record_field: VectorStoreField, field: Parameter) -> VectorStoreField:\n",
    "    if not record_field.name or record_field.name != field.name:\n",
    "        record_field.name = field.name\n",
    "    if not record_field.type_ and hasattr(field.annotation, \"__origin__\"):\n",
    "        property_type = field.annotation.__origin__\n",
    "        if record_field.field_type == FieldTypes.VECTOR:\n",
    "            if args := getattr(property_type, \"__args__\", None):\n",
    "                if NoneType in args and len(args) > 1:\n",
    "                    for arg in args:\n",
    "                        if arg is NoneType:\n",
    "                            continue\n",
    "\n",
    "                        if (\n",
    "                            (inner_args := getattr(arg, \"__args__\", None))\n",
    "                            and len(inner_args) == 1\n",
    "                            and inner_args[0] is not NoneType\n",
    "                        ):\n",
    "                            property_type = inner_args[0]\n",
    "                            break\n",
    "                        property_type = arg\n",
    "                        break\n",
    "                else:\n",
    "                    property_type = args[0]\n",
    "\n",
    "        else:\n",
    "            if (args := getattr(property_type, \"__args__\", None)) and NoneType in args and len(args) == 2:\n",
    "                property_type = args[0]\n",
    "\n",
    "        record_field.type_ = str(property_type) if hasattr(property_type, \"__args__\") else property_type.__name__\n",
    "\n",
    "    return record_field\n",
    "\n",
    "\n",
    "def _parse_parameter_to_field(field: Parameter) -> VectorStoreField | None:\n",
    "    # first check if there are any annotations\n",
    "    if field.annotation is not _empty and hasattr(field.annotation, \"__metadata__\"):\n",
    "        for field_annotation in field.annotation.__metadata__:\n",
    "            if isinstance(field_annotation, VectorStoreField):\n",
    "                return _parse_vector_store_record_field_instance(field_annotation, field)\n",
    "    # This means there are no annotations or that all annotations are of other types.\n",
    "    # we will check if there is a default, otherwise this will cause a runtime error.\n",
    "    # because it will not be stored, and retrieving this object will fail without a default for this field.\n",
    "    if field.default is _empty:\n",
    "        raise VectorStoreModelException(\n",
    "            \"Fields that do not have a VectorStoreField annotation must have a default value.\"\n",
    "        )\n",
    "    logger.debug(f'Field \"{field.name}\" does not have a VectorStoreField annotation, will not be part of the record.')\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_signature_to_definition(\n",
    "    parameters: MappingProxyType[str, Parameter], collection_name: str | None = None\n",
    ") -> VectorStoreCollectionDefinition:\n",
    "    if len(parameters) == 0:\n",
    "        raise VectorStoreModelException(\n",
    "            \"There must be at least one field in the datamodel. If you are using this with a @dataclass, \"\n",
    "            \"you might have inverted the order of the decorators, the vectorstoremodel decorator should be the top one.\"\n",
    "        )\n",
    "    fields = []\n",
    "    for param in parameters.values():\n",
    "        field = _parse_parameter_to_field(param)\n",
    "        if field:\n",
    "            fields.append(field)\n",
    "\n",
    "    return VectorStoreCollectionDefinition(\n",
    "        fields=fields,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "def vectorstoremodel(\n",
    "    cls: type[TModel] | None = None,\n",
    "    collection_name: str | None = None,\n",
    ") -> type[TModel]:\n",
    "    \"\"\"Returns the class as a vector store model.\n",
    "\n",
    "    This decorator makes a class a vector store model.\n",
    "    There are three things being checked:\n",
    "    - The class must have at least one field with a annotation,\n",
    "        of type VectorStoreField.\n",
    "    - The class must have exactly one field with the field_type `key`.\n",
    "    - When creating a Vector Field, either supply the property type directly,\n",
    "    or make sure to set the property that you want the index to use first.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        cls: The class to be decorated.\n",
    "        collection_name: The name of the collection to be used.\n",
    "            This is used to set the collection name in the VectorStoreCollectionDefinition.\n",
    "\n",
    "    Raises:\n",
    "        VectorStoreModelException: If there are no fields with a VectorStoreField annotation.\n",
    "        VectorStoreModelException: If there are fields with no name.\n",
    "        VectorStoreModelException: If there is no key field.\n",
    "    \"\"\"\n",
    "\n",
    "    def wrap(cls: type[TModel]) -> type[TModel]:\n",
    "        # get fields and annotations\n",
    "        cls_sig = signature(cls)\n",
    "        setattr(cls, \"__kernel_vectorstoremodel__\", True)\n",
    "        setattr(\n",
    "            cls,\n",
    "            \"__kernel_vectorstoremodel_definition__\",\n",
    "            _parse_signature_to_definition(cls_sig.parameters, collection_name),\n",
    "        )\n",
    "\n",
    "        return cls  # type: ignore\n",
    "\n",
    "    # See if we're being called as @vectorstoremodel or @vectorstoremodel().\n",
    "    if cls is None:\n",
    "        # We're called with parens.\n",
    "        return wrap  # type: ignore\n",
    "\n",
    "    # We're called as @vectorstoremodel without parens.\n",
    "    return wrap(cls)\n",
    "\n",
    "\n",
    "# region: VectorSearch Helpers\n",
    "\n",
    "\n",
    "def _get_collection_name_from_model(\n",
    "    record_type: type[TModel],\n",
    "    definition: VectorStoreCollectionDefinition | None = None,\n",
    ") -> str | None:\n",
    "    \"\"\"Get the collection name from the data model type or definition.\"\"\"\n",
    "    if record_type and not definition:\n",
    "        definition = getattr(record_type, \"__kernel_vectorstoremodel_definition__\", None)\n",
    "    if definition and definition.collection_name:\n",
    "        return definition.collection_name\n",
    "    return None\n",
    "\n",
    "\n",
    "@pyd_dataclass\n",
    "class GetFilteredRecordOptions:\n",
    "    \"\"\"Options for filtering records.\n",
    "\n",
    "    Args:\n",
    "        top: The maximum number of records to return.\n",
    "        skip: The number of records to skip.\n",
    "        order_by: A dictionary with fields names and a bool, True means ascending, False means descending.\n",
    "    \"\"\"\n",
    "\n",
    "    top: int = 10\n",
    "    skip: int = 0\n",
    "    order_by: Mapping[str, bool] | None = None\n",
    "\n",
    "\n",
    "class LambdaVisitor(NodeVisitor, Generic[TFilters]):\n",
    "    \"\"\"Visitor class to visit the AST nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, lambda_parser: Callable[[expr], TFilters], output_filters: list[TFilters] | None = None) -> None:\n",
    "        \"\"\"Initialize the visitor with a lambda parser and output filters.\"\"\"\n",
    "        self.lambda_parser = lambda_parser\n",
    "        self.output_filters = output_filters if output_filters is not None else []\n",
    "\n",
    "    def visit_Lambda(self, node: Lambda) -> None:\n",
    "        \"\"\"This method is called when a lambda expression is found.\"\"\"\n",
    "        self.output_filters.append(self.lambda_parser(node.body))\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class SearchType(str, Enum):\n",
    "    \"\"\"Enumeration for search types.\n",
    "\n",
    "    Contains: vector and keyword_hybrid.\n",
    "    \"\"\"\n",
    "\n",
    "    VECTOR = \"vector\"\n",
    "    KEYWORD_HYBRID = \"keyword_hybrid\"\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorSearchOptions(SearchOptions):\n",
    "    \"\"\"Options for vector search, builds on TextSearchOptions.\n",
    "\n",
    "    When multiple filters are used, they are combined with an AND operator.\n",
    "    \"\"\"\n",
    "\n",
    "    vector_property_name: str | None = None\n",
    "    additional_property_name: str | None = None\n",
    "    top: Annotated[int, Field(gt=0)] = 3\n",
    "    include_vectors: bool = False\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorSearchResult(KernelBaseModel, Generic[TModel]):\n",
    "    \"\"\"The result of a vector search.\"\"\"\n",
    "\n",
    "    record: TModel\n",
    "    score: float | None = None\n",
    "\n",
    "\n",
    "# region: VectorStoreRecordHandler\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorStoreRecordHandler(KernelBaseModel, Generic[TKey, TModel]):\n",
    "    \"\"\"Vector Store Record Handler class.\n",
    "\n",
    "    This class is used to serialize and deserialize records to and from a vector store.\n",
    "    As well as validating the data model against the vector store.\n",
    "    It is subclassed by VectorStoreRecordCollection and VectorSearchBase.\n",
    "    \"\"\"\n",
    "\n",
    "    record_type: type[TModel]\n",
    "    definition: VectorStoreCollectionDefinition\n",
    "    supported_key_types: ClassVar[set[str] | None] = None\n",
    "    supported_vector_types: ClassVar[set[str] | None] = None\n",
    "    embedding_generator: EmbeddingGeneratorBase | None = None\n",
    "\n",
    "    @property\n",
    "    def _key_field_name(self) -> str:\n",
    "        return self.definition.key_name\n",
    "\n",
    "    @property\n",
    "    def _key_field_storage_name(self) -> str:\n",
    "        return self.definition.key_field.storage_name or self.definition.key_name\n",
    "\n",
    "    @property\n",
    "    def _container_mode(self) -> bool:\n",
    "        return self.definition.container_mode\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def _ensure_definition(cls: type[_T], data: Any) -> dict[str, Any]:\n",
    "        \"\"\"Ensure there is a  data model definition, if it isn't passed, try to get it from the data model type.\"\"\"\n",
    "        if isinstance(data, dict) and not data.get(\"definition\"):\n",
    "            data[\"definition\"] = getattr(data[\"record_type\"], \"__kernel_vectorstoremodel_definition__\", None)\n",
    "        return data\n",
    "\n",
    "    def model_post_init(self, __context: object | None = None):\n",
    "        \"\"\"Post init function that sets the key field and container mode values, and validates the datamodel.\"\"\"\n",
    "        self._validate_data_model()\n",
    "\n",
    "    def _validate_data_model(self):\n",
    "        \"\"\"Internal function that can be overloaded by child classes to validate datatypes, etc.\n",
    "\n",
    "        This should take the VectorStoreRecordDefinition from the item_type and validate it against the store.\n",
    "\n",
    "        Checks can include, allowed naming of parameters, allowed data types, allowed vector dimensions.\n",
    "\n",
    "        Default checks are that the key field is in the allowed key types and the vector fields\n",
    "        are in the allowed vector types.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelValidationError: If the key field is not in the allowed key types.\n",
    "            VectorStoreModelValidationError: If the vector fields are not in the allowed vector types.\n",
    "\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.supported_key_types\n",
    "            and self.definition.key_field.type_\n",
    "            and self.definition.key_field.type_ not in self.supported_key_types\n",
    "        ):\n",
    "            raise VectorStoreModelValidationError(\n",
    "                f\"Key field must be one of {self.supported_key_types}, got {self.definition.key_field.type_}\"\n",
    "            )\n",
    "        if not self.supported_vector_types:\n",
    "            return\n",
    "        for field in self.definition.vector_fields:\n",
    "            if field.type_ and field.type_ not in self.supported_vector_types:\n",
    "                raise VectorStoreModelValidationError(\n",
    "                    f\"Vector field {field.name} must be one of {self.supported_vector_types}, got {field.type_}\"\n",
    "                )\n",
    "\n",
    "    @abstractmethod\n",
    "    def _serialize_dicts_to_store_models(self, records: Sequence[dict[str, Any]], **kwargs: Any) -> Sequence[Any]:\n",
    "        \"\"\"Serialize a list of dicts of the data to the store model.\n",
    "\n",
    "        This method should be overridden by the child class to convert the dict to the store model.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    def _deserialize_store_models_to_dicts(self, records: Sequence[Any], **kwargs: Any) -> Sequence[dict[str, Any]]:\n",
    "        \"\"\"Deserialize the store models to a list of dicts.\n",
    "\n",
    "        This method should be overridden by the child class to convert the store model to a list of dicts.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    # region Serialization methods\n",
    "\n",
    "    async def serialize(self, records: OneOrMany[TModel], **kwargs: Any) -> OneOrMany[Any]:\n",
    "        \"\"\"Serialize the data model to the store model.\n",
    "\n",
    "        This method follows the following steps:\n",
    "        1. Check if the data model has a serialize method.\n",
    "            Use that method to serialize and return the result.\n",
    "        2. Serialize the records into a dict, using the data model specific method.\n",
    "        3. Convert the dict to the store model, using the store specific method.\n",
    "\n",
    "        If overriding this method, make sure to first try to serialize the data model to the store model,\n",
    "        before doing the store specific version,\n",
    "        the user supplied version should have precedence.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelSerializationException: If an error occurs during serialization.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if serialized := self._serialize_data_model_to_store_model(records):\n",
    "                return serialized\n",
    "        except VectorStoreModelSerializationException:\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreModelSerializationException(f\"Error serializing records: {exc}\") from exc\n",
    "\n",
    "        try:\n",
    "            dict_records: list[dict[str, Any]] = []\n",
    "            if not isinstance(records, list):\n",
    "                records = [records]  # type: ignore\n",
    "            for rec in records:\n",
    "                dict_rec = self._serialize_data_model_to_dict(rec)\n",
    "                if isinstance(dict_rec, list):\n",
    "                    dict_records.extend(dict_rec)\n",
    "                else:\n",
    "                    dict_records.append(dict_rec)\n",
    "        except VectorStoreModelSerializationException:\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreModelSerializationException(f\"Error serializing records: {exc}\") from exc\n",
    "\n",
    "        # add vectors\n",
    "        try:\n",
    "            dict_records = await self._add_vectors_to_records(dict_records)  # type: ignore\n",
    "        except (VectorStoreModelException, VectorStoreOperationException):\n",
    "            raise\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreOperationException(\n",
    "                \"Exception occurred while trying to add the vectors to the records.\"\n",
    "            ) from exc\n",
    "\n",
    "        try:\n",
    "            return self._serialize_dicts_to_store_models(dict_records, **kwargs)  # type: ignore\n",
    "        except VectorStoreModelSerializationException:\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreModelSerializationException(f\"Error serializing records: {exc}\") from exc\n",
    "\n",
    "    def _serialize_data_model_to_store_model(self, record: OneOrMany[TModel], **kwargs: Any) -> OneOrMany[Any] | None:\n",
    "        \"\"\"Serialize the data model to the store model.\n",
    "\n",
    "        This works when the data model has supplied a serialize method, specific to a data source.\n",
    "        This is a method called 'serialize()' on the data model or part of the vector store record definition.\n",
    "\n",
    "        The developer is responsible for correctly serializing for the specific data source.\n",
    "        \"\"\"\n",
    "        if isinstance(record, Sequence):\n",
    "            result = [self._serialize_data_model_to_store_model(rec, **kwargs) for rec in record]\n",
    "            if not all(result):\n",
    "                return None\n",
    "            return result\n",
    "        if self.definition.serialize:\n",
    "            return self.definition.serialize(record, **kwargs)\n",
    "        if isinstance(record, SerializeMethodProtocol):\n",
    "            return record.serialize(**kwargs)\n",
    "        return None\n",
    "\n",
    "    def _serialize_data_model_to_dict(self, record: TModel, **kwargs: Any) -> OneOrList[dict[str, Any]]:\n",
    "        \"\"\"This function is used if no serialize method is found on the data model.\n",
    "\n",
    "        This will generally serialize the data model to a dict, should not be overridden by child classes.\n",
    "\n",
    "        The output of this should be passed to the serialize_dict_to_store_model method.\n",
    "        \"\"\"\n",
    "        if self.definition.to_dict:\n",
    "            return self.definition.to_dict(record, **kwargs)  # type: ignore\n",
    "        if isinstance(record, BaseModel):\n",
    "            return record.model_dump()\n",
    "\n",
    "        store_model = {}\n",
    "        for field in self.definition.fields:\n",
    "            store_model[field.storage_name or field.name] = (\n",
    "                record.get(field.name, None) if isinstance(record, Mapping) else getattr(record, field.name)\n",
    "            )\n",
    "        return store_model\n",
    "\n",
    "    # region Deserialization methods\n",
    "\n",
    "    def deserialize(self, records: OneOrMany[Any | dict[str, Any]], **kwargs: Any) -> OneOrMany[TModel] | None:\n",
    "        \"\"\"Deserialize the store model to the data model.\n",
    "\n",
    "        This method follows the following steps:\n",
    "        1. Check if the data model has a deserialize method.\n",
    "            Use that method to deserialize and return the result.\n",
    "        2. Deserialize the store model to a dict, using the store specific method.\n",
    "        3. Convert the dict to the data model, using the data model specific method.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not records:\n",
    "                return None\n",
    "            if deserialized := self._deserialize_store_model_to_data_model(records, **kwargs):\n",
    "                return deserialized\n",
    "\n",
    "            if isinstance(records, Sequence):\n",
    "                dict_records = self._deserialize_store_models_to_dicts(records, **kwargs)\n",
    "                return (\n",
    "                    self._deserialize_dict_to_data_model(dict_records, **kwargs)\n",
    "                    if self._container_mode\n",
    "                    else [self._deserialize_dict_to_data_model(rec, **kwargs) for rec in dict_records]\n",
    "                )\n",
    "\n",
    "            dict_record = self._deserialize_store_models_to_dicts([records], **kwargs)[0]\n",
    "            # regardless of mode, only 1 object is returned.\n",
    "            return self._deserialize_dict_to_data_model(dict_record, **kwargs)\n",
    "        except VectorStoreModelDeserializationException:\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreModelDeserializationException(f\"Error deserializing records: {exc}\") from exc\n",
    "\n",
    "    def _deserialize_store_model_to_data_model(self, record: OneOrMany[Any], **kwargs: Any) -> OneOrMany[TModel] | None:\n",
    "        \"\"\"Deserialize the store model to the data model.\n",
    "\n",
    "        This works when the data model has supplied a deserialize method, specific to a data source.\n",
    "        This uses a method called 'deserialize()' on the data model or part of the vector store record definition.\n",
    "\n",
    "        The developer is responsible for correctly deserializing for the specific data source.\n",
    "        \"\"\"\n",
    "        if self.definition.deserialize:\n",
    "            if isinstance(record, Sequence):\n",
    "                return self.definition.deserialize(record, **kwargs)\n",
    "            return self.definition.deserialize([record], **kwargs)\n",
    "        if func := getattr(self.record_type, \"deserialize\", None):\n",
    "            if isinstance(record, Sequence):\n",
    "                return [func(rec, **kwargs) for rec in record]\n",
    "            return func(record, **kwargs)\n",
    "        return None\n",
    "\n",
    "    def _deserialize_dict_to_data_model(self, record: OneOrMany[dict[str, Any]], **kwargs: Any) -> TModel:\n",
    "        \"\"\"This function is used if no deserialize method is found on the data model.\n",
    "\n",
    "        This method is the second step and will deserialize a dict to the data model,\n",
    "        should not be overridden by child classes.\n",
    "\n",
    "        The input of this should come from the _deserialized_store_model_to_dict function.\n",
    "        \"\"\"\n",
    "        if self.definition.from_dict:\n",
    "            if isinstance(record, Sequence):\n",
    "                return self.definition.from_dict(record, **kwargs)\n",
    "            ret = self.definition.from_dict([record], **kwargs)\n",
    "            return ret if self._container_mode else ret[0]\n",
    "        if isinstance(record, Sequence):\n",
    "            if len(record) > 1:\n",
    "                raise VectorStoreModelDeserializationException(\n",
    "                    \"Cannot deserialize multiple records to a single record unless you are using a container.\"\n",
    "                )\n",
    "            record = record[0]\n",
    "        if func := getattr(self.record_type, \"from_dict\", None):\n",
    "            return func(record)\n",
    "        if issubclass(self.record_type, BaseModel):\n",
    "            for field in self.definition.fields:\n",
    "                if field.storage_name and field.storage_name in record:\n",
    "                    record[field.name] = record.pop(field.storage_name)\n",
    "            return self.record_type.model_validate(record)  # type: ignore\n",
    "        data_model_dict: dict[str, Any] = {}\n",
    "        for field in self.definition.fields:\n",
    "            value = record.get(field.storage_name or field.name, None)\n",
    "            if field.field_type == FieldTypes.VECTOR and not kwargs.get(\"include_vectors\"):\n",
    "                continue\n",
    "            data_model_dict[field.name] = value\n",
    "        if self.record_type is dict:\n",
    "            return data_model_dict  # type: ignore\n",
    "        return self.record_type(**data_model_dict)\n",
    "\n",
    "    async def _add_vectors_to_records(\n",
    "        self,\n",
    "        records: OneOrMany[dict[str, Any]],\n",
    "        **kwargs,\n",
    "    ) -> OneOrMany[dict[str, Any]]:\n",
    "        \"\"\"Vectorize the vector record.\n",
    "\n",
    "        This function can be passed to upsert or upsert batch of a VectorStoreRecordCollection.\n",
    "\n",
    "        Loops through the fields of the data model definition,\n",
    "        looks at data fields, if they have a vector field,\n",
    "        looks up that vector field and checks if is a local embedding.\n",
    "\n",
    "        If so adds that to a list of embeddings to make.\n",
    "\n",
    "        Finally calls Kernel add_embedding_to_object with the list of embeddings to make.\n",
    "\n",
    "        Optional arguments are passed onto the Kernel add_embedding_to_object call.\n",
    "        \"\"\"\n",
    "        # dict of embedding_field.name and tuple of record, settings, field_name\n",
    "        embeddings_to_make: list[tuple[str, int, EmbeddingGeneratorBase]] = []\n",
    "\n",
    "        for field in self.definition.vector_fields:\n",
    "            embedding_generator = field.embedding_generator or self.embedding_generator\n",
    "            if not embedding_generator:\n",
    "                continue\n",
    "            if field.dimensions is None:\n",
    "                raise VectorStoreModelException(\n",
    "                    f\"Field {field.name} has no dimensions, cannot create embedding for field.\"\n",
    "                )\n",
    "            embeddings_to_make.append((\n",
    "                field.storage_name or field.name,\n",
    "                field.dimensions,\n",
    "                embedding_generator,\n",
    "            ))\n",
    "\n",
    "        for field_name, dimensions, embedder in embeddings_to_make:\n",
    "            await self._add_embedding_to_object(\n",
    "                inputs=records,\n",
    "                field_name=field_name,\n",
    "                dimensions=dimensions,\n",
    "                embedding_generator=embedder,\n",
    "                container_mode=self.definition.container_mode,\n",
    "                **kwargs,\n",
    "            )\n",
    "        return records\n",
    "\n",
    "    async def _add_embedding_to_object(\n",
    "        self,\n",
    "        inputs: OneOrMany[Any],\n",
    "        field_name: str,\n",
    "        dimensions: int,\n",
    "        embedding_generator: EmbeddingGeneratorBase,\n",
    "        container_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Gather all fields to embed, batch the embedding generation and store.\"\"\"\n",
    "        contents: list[Any] = []\n",
    "        dict_like = (getter := getattr(inputs, \"get\", False)) and callable(getter)\n",
    "        list_of_dicts: bool = False\n",
    "        if isinstance(inputs, list):\n",
    "            list_of_dicts = (getter := getattr(inputs[0], \"get\", False)) and callable(getter)\n",
    "            for record in inputs:\n",
    "                if list_of_dicts:\n",
    "                    contents.append(record.get(field_name))  # type: ignore\n",
    "                else:\n",
    "                    contents.append(getattr(record, field_name))\n",
    "        else:\n",
    "            if dict_like:\n",
    "                contents.append(inputs.get(field_name))  # type: ignore\n",
    "            else:\n",
    "                contents.append(getattr(inputs, field_name))\n",
    "\n",
    "        vectors = await embedding_generator.generate_raw_embeddings(\n",
    "            texts=contents, settings=PromptExecutionSettings(dimensions=dimensions), **kwargs\n",
    "        )  # type: ignore\n",
    "        if vectors is None:\n",
    "            raise VectorStoreOperationException(\"No vectors were generated.\")\n",
    "        if isinstance(inputs, list):\n",
    "            for record, vector in zip(inputs, vectors):\n",
    "                if list_of_dicts:\n",
    "                    record[field_name] = vector  # type: ignore\n",
    "                else:\n",
    "                    setattr(record, field_name, vector)\n",
    "            return\n",
    "        if dict_like:\n",
    "            inputs[field_name] = vectors[0]  # type: ignore\n",
    "            return\n",
    "        setattr(inputs, field_name, vectors[0])\n",
    "\n",
    "\n",
    "# region: VectorStoreRecordCollection\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorStoreCollection(VectorStoreRecordHandler[TKey, TModel], Generic[TKey, TModel]):\n",
    "    \"\"\"Base class for a vector store record collection.\"\"\"\n",
    "\n",
    "    collection_name: str = \"\"\n",
    "    managed_client: bool = True\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def _ensure_collection_name(cls: type[_T], data: Any) -> dict[str, Any]:\n",
    "        \"\"\"Ensure there is a collection name, if it isn't passed, try to get it from the data model type.\"\"\"\n",
    "        if (\n",
    "            isinstance(data, dict)\n",
    "            and not data.get(\"collection_name\")\n",
    "            and (collection_name := _get_collection_name_from_model(data[\"record_type\"], data.get(\"definition\")))\n",
    "        ):\n",
    "            data[\"collection_name\"] = collection_name\n",
    "        return data\n",
    "\n",
    "    async def __aenter__(self) -> Self:\n",
    "        \"\"\"Enter the context manager.\"\"\"\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_value, traceback) -> None:\n",
    "        \"\"\"Exit the context manager.\n",
    "\n",
    "        Should be overridden by subclasses, if necessary.\n",
    "\n",
    "        If the client is passed in the constructor, it should not be closed,\n",
    "        in that case the managed_client should be set to False.\n",
    "\n",
    "        If the store supplied the managed client, it is responsible for closing it,\n",
    "        and it should not be closed here and so managed_client should be False.\n",
    "\n",
    "        Some services use two clients, one for the store and one for the collection,\n",
    "        in that case, the collection client should be closed here,\n",
    "        but the store client should only be closed when it is created in the collection.\n",
    "        A additional flag might be needed for that.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _inner_upsert(\n",
    "        self,\n",
    "        records: Sequence[Any],\n",
    "        **kwargs: Any,\n",
    "    ) -> Sequence[TKey]:\n",
    "        \"\"\"Upsert the records, this should be overridden by the child class.\n",
    "\n",
    "        Args:\n",
    "            records: The records, the format is specific to the store.\n",
    "            **kwargs (Any): Additional arguments, to be passed to the store.\n",
    "\n",
    "        Returns:\n",
    "            The keys of the upserted records.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the upsert.\n",
    "                There is no need to catch and parse exceptions in the inner functions,\n",
    "                they are handled by the public methods.\n",
    "                The only exception is raises exceptions yourself, such as a ValueError.\n",
    "                This is then caught and turned into the relevant exception by the public method.\n",
    "                This setup promotes a limited depth of the stack trace.\n",
    "\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _inner_get(\n",
    "        self,\n",
    "        keys: Sequence[TKey] | None = None,\n",
    "        options: GetFilteredRecordOptions | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> OneOrMany[Any] | None:\n",
    "        \"\"\"Get the records, this should be overridden by the child class.\n",
    "\n",
    "        Args:\n",
    "            keys: The keys to get.\n",
    "            options: the options to use for the get.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records from the store, not deserialized.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the upsert.\n",
    "                There is no need to catch and parse exceptions in the inner functions,\n",
    "                they are handled by the public methods.\n",
    "                The only exception is raises exceptions yourself, such as a ValueError.\n",
    "                This is then caught and turned into the relevant exception by the public method.\n",
    "                This setup promotes a limited depth of the stack trace.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _inner_delete(self, keys: Sequence[TKey], **kwargs: Any) -> None:\n",
    "        \"\"\"Delete the records, this should be overridden by the child class.\n",
    "\n",
    "        Args:\n",
    "            keys: The keys.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the upsert.\n",
    "                There is no need to catch and parse exceptions in the inner functions,\n",
    "                they are handled by the public methods.\n",
    "                The only exception is raises exceptions yourself, such as a ValueError.\n",
    "                This is then caught and turned into the relevant exception by the public method.\n",
    "                This setup promotes a limited depth of the stack trace.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def ensure_collection_exists(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Create the collection in the service.\n",
    "\n",
    "        This should be overridden by the child class. Should first check if the collection exists,\n",
    "        if it does not, it should create the collection.\n",
    "\n",
    "        Raises:\n",
    "            Make sure the implementation of this function raises relevant exceptions with good descriptions.\n",
    "            This is different then the `_inner_x` methods, as this is a public method.\n",
    "\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def collection_exists(self, **kwargs: Any) -> bool:\n",
    "        \"\"\"Check if the collection exists.\n",
    "\n",
    "        This should be overridden by the child class.\n",
    "\n",
    "        Raises:\n",
    "            Make sure the implementation of this function raises relevant exceptions with good descriptions.\n",
    "            This is different then the `_inner_x` methods, as this is a public method.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def ensure_collection_deleted(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Delete the collection.\n",
    "\n",
    "        This should be overridden by the child class.\n",
    "\n",
    "        Raises:\n",
    "            Make sure the implementation of this function raises relevant exceptions with good descriptions.\n",
    "            This is different then the `_inner_x` methods, as this is a public method.\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    async def upsert(\n",
    "        self,\n",
    "        records: OneOrMany[TModel],\n",
    "        **kwargs,\n",
    "    ) -> OneOrMany[TKey]:\n",
    "        \"\"\"Upsert one or more records.\n",
    "\n",
    "        If the key of the record already exists, the existing record will be updated.\n",
    "        If the key does not exist, a new record will be created.\n",
    "\n",
    "        Args:\n",
    "            records: The records to upsert, can be a single record, a list of records, or a single container.\n",
    "                If a single record is passed, a single key is returned, instead of a list of keys.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            OneOrMany[TKey]: The keys of the upserted records.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelSerializationException: If an error occurs during serialization.\n",
    "            VectorStoreOperationException: If an error occurs during upserting.\n",
    "        \"\"\"\n",
    "        batch = True\n",
    "        if not isinstance(records, list) and not self._container_mode:\n",
    "            batch = False\n",
    "        if records is None:\n",
    "            raise VectorStoreOperationException(\"Either record or records must be provided.\")\n",
    "\n",
    "        try:\n",
    "            data = await self.serialize(records)\n",
    "        # the serialize method will parse any exception into a VectorStoreModelSerializationException\n",
    "        except VectorStoreModelSerializationException:\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            results = await self._inner_upsert(data if isinstance(data, list) else [data], **kwargs)  # type: ignore\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreOperationException(f\"Error upserting record(s): {exc}\") from exc\n",
    "        if batch or self._container_mode:\n",
    "            return results\n",
    "        return results[0]\n",
    "\n",
    "    @overload\n",
    "    async def get(\n",
    "        self,\n",
    "        top: int = ...,\n",
    "        skip: int = ...,\n",
    "        order_by: OneOrMany[str] | dict[str, bool] | None = None,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Sequence[TModel] | None:\n",
    "        \"\"\"Get records based on the ordering and selection criteria.\n",
    "\n",
    "        Args:\n",
    "            include_vectors: Include the vectors in the response. Default is True.\n",
    "                Some vector stores do not support retrieving without vectors, even when set to false.\n",
    "                Some vector stores have specific parameters to control that behavior, when\n",
    "                that parameter is set, include_vectors is ignored.\n",
    "            top: The number of records to return.\n",
    "                Only used if keys are not provided.\n",
    "            skip: The number of records to skip.\n",
    "                Only used if keys are not provided.\n",
    "            order_by: The order by clause,\n",
    "                this can be a string, a list of strings or a dict,\n",
    "                when passing strings, they are assumed to be ascending.\n",
    "                Otherwise, use the value in the dict to set ascending (True) or descending (False).\n",
    "                example: {\"field_name\": True} or [\"field_name\", {\"field_name2\": False}].\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records, either a list of TModel or the container type.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during the get.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    async def get(\n",
    "        self,\n",
    "        key: TKey = ...,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> TModel | None:\n",
    "        \"\"\"Get a record if it exists.\n",
    "\n",
    "        Args:\n",
    "            key: The key to get.\n",
    "            include_vectors: Include the vectors in the response. Default is True.\n",
    "                Some vector stores do not support retrieving without vectors, even when set to false.\n",
    "                Some vector stores have specific parameters to control that behavior, when\n",
    "                that parameter is set, include_vectors is ignored.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records, either a list of TModel or the container type.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during the get.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    async def get(\n",
    "        self,\n",
    "        keys: Sequence[TKey] = ...,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> OneOrMany[TModel] | None:\n",
    "        \"\"\"Get a batch of records whose keys exist in the collection, i.e. keys that do not exist are ignored.\n",
    "\n",
    "        Args:\n",
    "            keys: The keys to get, if keys are provided, key is ignored.\n",
    "            include_vectors: Include the vectors in the response. Default is True.\n",
    "                Some vector stores do not support retrieving without vectors, even when set to false.\n",
    "                Some vector stores have specific parameters to control that behavior, when\n",
    "                that parameter is set, include_vectors is ignored.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records, either a list of TModel or the container type.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during the get.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        key=None,\n",
    "        keys=None,\n",
    "        include_vectors=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Get a batch of records whose keys exist in the collection, i.e. keys that do not exist are ignored.\n",
    "\n",
    "        Args:\n",
    "            key: The key to get.\n",
    "            keys: The keys to get, if keys are provided, key is ignored.\n",
    "            include_vectors: Include the vectors in the response. Default is True.\n",
    "                Some vector stores do not support retrieving without vectors, even when set to false.\n",
    "                Some vector stores have specific parameters to control that behavior, when\n",
    "                that parameter is set, include_vectors is ignored.\n",
    "            top: The number of records to return.\n",
    "                Only used if keys are not provided.\n",
    "            skip: The number of records to skip.\n",
    "                Only used if keys are not provided.\n",
    "            order_by: The order by clause, this is a list of dicts with the field name and ascending flag,\n",
    "                (default is True, which means ascending).\n",
    "                Only used if keys are not provided.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records, either a list of TModel or the container type.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during the get.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        batch = True\n",
    "        options = None\n",
    "        if not keys and key:\n",
    "            if not isinstance(key, list):\n",
    "                keys = [key]\n",
    "                batch = False\n",
    "            else:\n",
    "                keys = key\n",
    "        if not keys:\n",
    "            if kwargs:\n",
    "                get_args = {}\n",
    "                kw_order_by: OneOrList[str] | dict[str, bool] | None = kwargs.pop(\"order_by\", None)  # type: ignore\n",
    "                if \"top\" in kwargs:\n",
    "                    get_args[\"top\"] = kwargs.pop(\"top\", None)\n",
    "                if \"skip\" in kwargs:\n",
    "                    get_args[\"skip\"] = kwargs.pop(\"skip\", None)\n",
    "                order_by: dict[str, bool] | None = None  # type: ignore\n",
    "                if kw_order_by is not None:\n",
    "                    order_by = {}\n",
    "                    if isinstance(kw_order_by, str):\n",
    "                        order_by[kw_order_by] = True\n",
    "                    elif isinstance(kw_order_by, dict):\n",
    "                        order_by = kw_order_by\n",
    "                    elif isinstance(kw_order_by, list):\n",
    "                        for item in kw_order_by:\n",
    "                            if isinstance(item, str):\n",
    "                                order_by[item] = True\n",
    "                            else:\n",
    "                                order_by.update(item)\n",
    "                    else:\n",
    "                        raise VectorStoreOperationException(\n",
    "                            f\"Invalid order_by type: {type(order_by)}, expected str, dict or list.\"\n",
    "                        )\n",
    "                    get_args[\"order_by\"] = order_by\n",
    "                try:\n",
    "                    options = GetFilteredRecordOptions(**get_args)\n",
    "                except Exception as exc:\n",
    "                    raise VectorStoreOperationException(f\"Error creating options: {exc}\") from exc\n",
    "            else:\n",
    "                raise VectorStoreOperationException(\"Either key, keys or options must be provided.\")\n",
    "        try:\n",
    "            records = await self._inner_get(keys, include_vectors=include_vectors, options=options, **kwargs)\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreOperationException(f\"Error getting record(s): {exc}\") from exc\n",
    "\n",
    "        if not records:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            model_records = self.deserialize(\n",
    "                records if batch else records[0], include_vectors=include_vectors, **kwargs\n",
    "            )\n",
    "        # the deserialize method will parse any exception into a VectorStoreModelDeserializationException\n",
    "        except VectorStoreModelDeserializationException:\n",
    "            raise\n",
    "\n",
    "        # there are many code paths within the deserialize method, some supplied by the developer,\n",
    "        # and so depending on what is used,\n",
    "        # it might return a sequence, so we just return the first element,\n",
    "        # there should never be multiple elements (this is not a batch get),\n",
    "        # hence a raise if there are.\n",
    "        if batch:\n",
    "            return model_records\n",
    "        if not isinstance(model_records, Sequence):\n",
    "            return model_records\n",
    "        if len(model_records) == 1:\n",
    "            return model_records[0]\n",
    "        raise VectorStoreModelDeserializationException(\n",
    "            f\"Error deserializing record, multiple records returned: {model_records}\"\n",
    "        )\n",
    "\n",
    "    async def delete(self, keys: OneOrMany[TKey], **kwargs):\n",
    "        \"\"\"Delete one or more records by key.\n",
    "\n",
    "        An exception will be raised at the end if any record does not exist.\n",
    "\n",
    "        Args:\n",
    "            keys: The key or keys to be deleted.\n",
    "            **kwargs: Additional arguments.\n",
    "        Exceptions:\n",
    "            VectorStoreOperationException: If an error occurs during deletion or a record does not exist.\n",
    "        \"\"\"\n",
    "        if not isinstance(keys, list):\n",
    "            keys = [keys]  # type: ignore\n",
    "        try:\n",
    "            await self._inner_delete(keys, **kwargs)  # type: ignore\n",
    "        except Exception as exc:\n",
    "            raise VectorStoreOperationException(f\"Error deleting record(s): {exc}\") from exc\n",
    "\n",
    "\n",
    "# region: VectorStore\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorStore(KernelBaseModel):\n",
    "    \"\"\"Base class for vector stores.\"\"\"\n",
    "\n",
    "    managed_client: bool = True\n",
    "    embedding_generator: EmbeddingGeneratorBase | None = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_collection(\n",
    "        self,\n",
    "        record_type: type[TModel],\n",
    "        *,\n",
    "        definition: VectorStoreCollectionDefinition | None = None,\n",
    "        collection_name: str | None = None,\n",
    "        embedding_generator: EmbeddingGeneratorBase | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"VectorStoreCollection\":\n",
    "        \"\"\"Get a vector store record collection instance tied to this store.\n",
    "\n",
    "        Args:\n",
    "            record_type: The type of the records that will be used.\n",
    "            definition: The data model definition.\n",
    "            collection_name: The name of the collection.\n",
    "            embedding_generator: The embedding generator to use.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            A vector store record collection instance tied to this store.\n",
    "\n",
    "        \"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    @abstractmethod\n",
    "    async def list_collection_names(self, **kwargs) -> Sequence[str]:\n",
    "        \"\"\"Get the names of all collections.\"\"\"\n",
    "        ...  # pragma: no cover\n",
    "\n",
    "    async def collection_exists(self, collection_name: str) -> bool:\n",
    "        \"\"\"Check if a collection exists.\n",
    "\n",
    "        This is a wrapper around the get_collection method of a collection,\n",
    "        to check if the collection exists.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_model = VectorStoreCollectionDefinition(fields=[VectorStoreField(\"key\", name=\"id\")])\n",
    "            collection = self.get_collection(record_type=dict, definition=data_model, collection_name=collection_name)\n",
    "            return await collection.collection_exists()\n",
    "        except VectorStoreOperationException:\n",
    "            return False\n",
    "\n",
    "    async def ensure_collection_deleted(self, collection_name: str) -> None:\n",
    "        \"\"\"Delete a collection.\n",
    "\n",
    "        This is a wrapper around the get_collection method of a collection,\n",
    "        to delete the collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_model = VectorStoreCollectionDefinition(fields=[VectorStoreField(\"key\", name=\"id\")])\n",
    "            collection = self.get_collection(record_type=dict, definition=data_model, collection_name=collection_name)\n",
    "            await collection.ensure_collection_deleted()\n",
    "        except VectorStoreOperationException:\n",
    "            pass\n",
    "\n",
    "    async def __aenter__(self) -> Self:\n",
    "        \"\"\"Enter the context manager.\"\"\"\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_value, traceback) -> None:\n",
    "        \"\"\"Exit the context manager.\n",
    "\n",
    "        Should be overridden by subclasses, if necessary.\n",
    "\n",
    "        If the client is passed in the constructor, it should not be closed,\n",
    "        in that case the managed_client should be set to False.\n",
    "        \"\"\"\n",
    "        pass  # pragma: no cover\n",
    "\n",
    "\n",
    "# region: Vector Search\n",
    "\n",
    "\n",
    "@release_candidate\n",
    "class VectorSearch(VectorStoreRecordHandler[TKey, TModel], Generic[TKey, TModel]):\n",
    "    \"\"\"Base class for searching vectors.\"\"\"\n",
    "\n",
    "    supported_search_types: ClassVar[set[SearchType]] = Field(default_factory=set)\n",
    "\n",
    "    @property\n",
    "    def options_class(self) -> type[SearchOptions]:\n",
    "        \"\"\"The options class for the search.\"\"\"\n",
    "        return VectorSearchOptions\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _inner_search(\n",
    "        self,\n",
    "        search_type: SearchType,\n",
    "        options: VectorSearchOptions,\n",
    "        values: Any | None = None,\n",
    "        vector: Sequence[float | int] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult[TModel]]:\n",
    "        \"\"\"Inner search method.\n",
    "\n",
    "        This is the main search method that should be implemented, and will be called by the public search methods.\n",
    "        Currently, at least one of the three search contents will be provided\n",
    "        (through the public interface mixin functions), in the future, this may be expanded to allow multiple of them.\n",
    "\n",
    "        This method should return a KernelSearchResults object with the results of the search.\n",
    "        The inner \"results\" object of the KernelSearchResults should be a async iterator that yields the search results,\n",
    "        this allows things like paging to be implemented.\n",
    "\n",
    "        There is a default helper method \"_get_vector_search_results_from_results\" to convert\n",
    "        the results to a async iterable VectorSearchResults, but this can be overridden if necessary.\n",
    "\n",
    "        Options might be a object of type VectorSearchOptions, or a subclass of it.\n",
    "\n",
    "        The implementation of this method must deal with the possibility that multiple search contents are provided,\n",
    "        and should handle them in a way that makes sense for that particular store.\n",
    "\n",
    "        The public methods will catch and reraise the three exceptions mentioned below, others are caught and turned\n",
    "        into a VectorSearchExecutionException.\n",
    "\n",
    "        Args:\n",
    "            search_type: The type of search to perform.\n",
    "            options: The search options, can be None.\n",
    "            values: The values to search for, optional.\n",
    "            vector: The vector to search for, optional.\n",
    "            **kwargs: Additional arguments that might be needed.\n",
    "\n",
    "        Returns:\n",
    "            The search results, wrapped in a KernelSearchResults object.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_record_from_result(self, result: Any) -> Any:\n",
    "        \"\"\"Get the record from the returned search result.\n",
    "\n",
    "        Does any unpacking or processing of the result to get just the record.\n",
    "\n",
    "        If the underlying SDK of the store returns a particular type that might include something\n",
    "        like a score or other metadata, this method should be overridden to extract just the record.\n",
    "\n",
    "        Likely returns a dict, but in some cases could return the record in the form of a SDK specific object.\n",
    "\n",
    "        This method is used as part of the _get_vector_search_results_from_results method,\n",
    "        the output of it is passed to the deserializer.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_score_from_result(self, result: Any) -> float | None:\n",
    "        \"\"\"Get the score from the result.\n",
    "\n",
    "        Does any unpacking or processing of the result to get just the score.\n",
    "\n",
    "        If the underlying SDK of the store returns a particular type with a score or other metadata,\n",
    "        this method extracts it.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def _get_vector_search_results_from_results(\n",
    "        self, results: AsyncIterable[Any] | Sequence[Any], options: VectorSearchOptions | None = None\n",
    "    ) -> AsyncIterable[VectorSearchResult[TModel]]:\n",
    "        if isinstance(results, Sequence):\n",
    "            results = desync_list(results)\n",
    "        async for result in results:\n",
    "            if not result:\n",
    "                continue\n",
    "            try:\n",
    "                record = self.deserialize(\n",
    "                    self._get_record_from_result(result), include_vectors=options.include_vectors if options else True\n",
    "                )\n",
    "            except VectorStoreModelDeserializationException:\n",
    "                raise\n",
    "            except Exception as exc:\n",
    "                raise VectorStoreModelDeserializationException(\n",
    "                    f\"An error occurred while deserializing the record: {exc}\"\n",
    "                ) from exc\n",
    "            score = self._get_score_from_result(result)\n",
    "            if record is not None:\n",
    "                # single records are always returned as single records by the deserializer\n",
    "                yield VectorSearchResult(record=record, score=score)  # type: ignore\n",
    "\n",
    "    @overload\n",
    "    async def search(\n",
    "        self,\n",
    "        values: Any,\n",
    "        *,\n",
    "        vector_field_name: str | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 3,\n",
    "        skip: int = 0,\n",
    "        include_total_count: bool = False,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult[TModel]]:\n",
    "        \"\"\"Search the vector store with Vector search for records that match the given value and filter.\n",
    "\n",
    "        Args:\n",
    "            values: The values to search for. These will be vectorized,\n",
    "                either by the store or using the provided generator.\n",
    "            vector_field_name: The name of the vector field to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    async def search(\n",
    "        self,\n",
    "        *,\n",
    "        vector: Sequence[float | int],\n",
    "        vector_field_name: str | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 3,\n",
    "        skip: int = 0,\n",
    "        include_total_count: bool = False,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult[TModel]]:\n",
    "        \"\"\"Search the vector store with Vector search for records that match the given vector and filter.\n",
    "\n",
    "        Args:\n",
    "            vector: The vector to search for\n",
    "            vector_field_name: The name of the vector field to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def search(\n",
    "        self,\n",
    "        values=None,\n",
    "        *,\n",
    "        vector=None,\n",
    "        vector_property_name=None,\n",
    "        filter=None,\n",
    "        top=3,\n",
    "        skip=0,\n",
    "        include_total_count=False,\n",
    "        include_vectors=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Search the vector store for records that match the given value and filter.\n",
    "\n",
    "        Args:\n",
    "            values: The values to search for.\n",
    "            vector: The vector to search for, if not provided, the values will be used to generate a vector.\n",
    "            vector_property_name: The name of the vector property to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        if SearchType.VECTOR not in self.supported_search_types:\n",
    "            raise VectorStoreOperationNotSupportedException(\n",
    "                f\"Vector search is not supported by this vector store: {self.__class__.__name__}\"\n",
    "            )\n",
    "        options = VectorSearchOptions(\n",
    "            filter=filter,\n",
    "            vector_property_name=vector_property_name,\n",
    "            top=top,\n",
    "            skip=skip,\n",
    "            include_total_count=include_total_count,\n",
    "            include_vectors=include_vectors,\n",
    "        )\n",
    "        try:\n",
    "            return await self._inner_search(\n",
    "                search_type=SearchType.VECTOR,\n",
    "                values=values,\n",
    "                options=options,\n",
    "                vector=vector,\n",
    "                **kwargs,\n",
    "            )\n",
    "        except (\n",
    "            VectorStoreModelDeserializationException,\n",
    "            VectorSearchOptionsException,\n",
    "            VectorSearchExecutionException,\n",
    "            VectorStoreOperationNotSupportedException,\n",
    "            VectorStoreOperationException,\n",
    "        ):\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorSearchExecutionException(f\"An error occurred during the search: {exc}\") from exc\n",
    "\n",
    "    async def hybrid_search(\n",
    "        self,\n",
    "        values: Any,\n",
    "        *,\n",
    "        vector: list[float | int] | None = None,\n",
    "        vector_property_name: str | None = None,\n",
    "        additional_property_name: str | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 3,\n",
    "        skip: int = 0,\n",
    "        include_total_count: bool = False,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult[TModel]]:\n",
    "        \"\"\"Search the vector store for records that match the given values and filter.\n",
    "\n",
    "        Args:\n",
    "            values: The values to search for.\n",
    "            vector: The vector to search for, if not provided, the values will be used to generate a vector.\n",
    "            vector_property_name: The name of the vector field to use for the search.\n",
    "            additional_property_name: The name of the additional property field to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        if SearchType.KEYWORD_HYBRID not in self.supported_search_types:\n",
    "            raise VectorStoreOperationNotSupportedException(\n",
    "                f\"Keyword hybrid search is not supported by this vector store: {self.__class__.__name__}\"\n",
    "            )\n",
    "        options = VectorSearchOptions(\n",
    "            filter=filter,\n",
    "            vector_property_name=vector_property_name,\n",
    "            additional_property_name=additional_property_name,\n",
    "            top=top,\n",
    "            skip=skip,\n",
    "            include_total_count=include_total_count,\n",
    "            include_vectors=include_vectors,\n",
    "        )\n",
    "        try:\n",
    "            return await self._inner_search(\n",
    "                search_type=SearchType.KEYWORD_HYBRID,\n",
    "                values=values,\n",
    "                vector=vector,\n",
    "                options=options,\n",
    "                **kwargs,\n",
    "            )\n",
    "        except (\n",
    "            VectorStoreModelDeserializationException,\n",
    "            VectorSearchOptionsException,\n",
    "            VectorSearchExecutionException,\n",
    "            VectorStoreOperationNotSupportedException,\n",
    "            VectorStoreOperationException,\n",
    "        ):\n",
    "            raise  # pragma: no cover\n",
    "        except Exception as exc:\n",
    "            raise VectorSearchExecutionException(f\"An error occurred during the search: {exc}\") from exc\n",
    "\n",
    "    async def _generate_vector_from_values(\n",
    "        self,\n",
    "        values: Any | None,\n",
    "        options: VectorSearchOptions,\n",
    "    ) -> Sequence[float | int] | None:\n",
    "        \"\"\"Generate a vector from the given keywords.\"\"\"\n",
    "        if values is None:\n",
    "            return None\n",
    "        vector_field = self.definition.try_get_vector_field(options.vector_property_name)\n",
    "        if not vector_field:\n",
    "            raise VectorSearchOptionsException(\n",
    "                f\"Vector field '{options.vector_property_name}' not found in data model definition.\"\n",
    "            )\n",
    "        embedding_generator = (\n",
    "            vector_field.embedding_generator if vector_field.embedding_generator else self.embedding_generator\n",
    "        )\n",
    "        if not embedding_generator:\n",
    "            raise VectorSearchOptionsException(\n",
    "                f\"Embedding generator not found for vector field '{options.vector_property_name}'.\"\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            await embedding_generator.generate_embeddings(\n",
    "                # TODO (eavanvalkenburg): this only deals with string values, should support other types as well\n",
    "                # but that requires work on the embedding generators first.\n",
    "                texts=[values if isinstance(values, str) else json.dumps(values)],\n",
    "                settings=PromptExecutionSettings(dimensions=vector_field.dimensions),\n",
    "            )\n",
    "        )[0].tolist()\n",
    "\n",
    "    def _build_filter(self, search_filter: OptionalOneOrMany[Callable | str] | None) -> OptionalOneOrMany[Any]:\n",
    "        \"\"\"Create the filter based on the filters.\n",
    "\n",
    "        This function returns None, a single filter, or a list of filters.\n",
    "        If a single filter is passed, a single filter is returned.\n",
    "\n",
    "        It takes the filters, which can be a Callable (lambda) or a string, and parses them into a filter object,\n",
    "        using the _lambda_parser method that is specific to each vector store.\n",
    "\n",
    "        If a list of filters, is passed, the parsed filters are also returned as a list, so the caller needs to\n",
    "        combine them in the appropriate way.\n",
    "\n",
    "        Often called like this (when filters are strings):\n",
    "        ```python\n",
    "        if filter := self._build_filter(options.filter):\n",
    "            search_args[\"filter\"] = filter if isinstance(filter, str) else \" and \".join(filter)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if not search_filter:\n",
    "            return None\n",
    "\n",
    "        filters = search_filter if isinstance(search_filter, list) else [search_filter]\n",
    "\n",
    "        created_filters: list[Any] = []\n",
    "\n",
    "        visitor = LambdaVisitor(self._lambda_parser)\n",
    "        for filter_ in filters:\n",
    "            # parse lambda expression with AST\n",
    "            tree = parse(filter_ if isinstance(filter_, str) else getsource(filter_).strip())\n",
    "            visitor.visit(tree)\n",
    "        created_filters = visitor.output_filters\n",
    "        if len(created_filters) == 0:\n",
    "            raise VectorStoreOperationException(\"No filter strings found.\")\n",
    "        if len(created_filters) == 1:\n",
    "            return created_filters[0]\n",
    "        return created_filters\n",
    "\n",
    "    @abstractmethod\n",
    "    def _lambda_parser(self, node: AST) -> Any:\n",
    "        \"\"\"Parse the lambda expression and return the filter string.\n",
    "\n",
    "        This follows from the ast specs: https://docs.python.org/3/library/ast.html\n",
    "        \"\"\"\n",
    "        # This method should be implemented in the derived class\n",
    "        # to parse the lambda expression and return the filter string.\n",
    "        pass\n",
    "\n",
    "    def create_search_function(\n",
    "        self,\n",
    "        function_name: str = DEFAULT_FUNCTION_NAME,\n",
    "        description: str = DEFAULT_DESCRIPTION,\n",
    "        *,\n",
    "        search_type: Literal[\"vector\", \"keyword_hybrid\"] = \"vector\",\n",
    "        parameters: list[KernelParameterMetadata] | None = None,\n",
    "        return_parameter: KernelParameterMetadata | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 5,\n",
    "        skip: int = 0,\n",
    "        vector_property_name: str | None = None,\n",
    "        additional_property_name: str | None = None,\n",
    "        include_vectors: bool = False,\n",
    "        include_total_count: bool = False,\n",
    "        filter_update_function: DynamicFilterFunction | None = None,\n",
    "        string_mapper: Callable[[VectorSearchResult[TModel]], str] | None = None,\n",
    "    ) -> KernelFunction:\n",
    "        \"\"\"Create a kernel function from a search function.\n",
    "\n",
    "        Args:\n",
    "            function_name: The name of the function, to be used in the kernel, default is \"search\".\n",
    "            description: The description of the function, a default is provided.\n",
    "            search_type: The type of search to perform, can be 'vector' or 'keyword_hybrid'.\n",
    "            parameters: The parameters for the function,\n",
    "                use an empty list for a function without parameters,\n",
    "                use None for the default set, which is \"query\", \"top\", and \"skip\".\n",
    "            return_parameter: The return parameter for the function.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            vector_property_name: The name of the vector property to use for the search.\n",
    "            additional_property_name: The name of the additional property field to use for the search.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            filter_update_function: A function to update the filters.\n",
    "                The function should return the updated filter.\n",
    "                The default function uses the parameters and the kwargs to update the filters, it\n",
    "                adds equal to filters to the options for all parameters that are not \"query\".\n",
    "                As well as adding equal to filters for parameters that have a default value.\n",
    "            string_mapper: The function to map the search results to strings.\n",
    "        \"\"\"\n",
    "        search_types = SearchType(search_type)\n",
    "        if search_types not in self.supported_search_types:\n",
    "            raise VectorStoreOperationNotSupportedException(\n",
    "                f\"Search type '{search_types.value}' is not supported by this vector store: {self.__class__.__name__}\"\n",
    "            )\n",
    "        options = VectorSearchOptions(\n",
    "            filter=filter,\n",
    "            skip=skip,\n",
    "            top=top,\n",
    "            include_total_count=include_total_count,\n",
    "            include_vectors=include_vectors,\n",
    "            vector_property_name=vector_property_name,\n",
    "            additional_property_name=additional_property_name,\n",
    "        )\n",
    "        return self._create_kernel_function(\n",
    "            search_type=search_types,\n",
    "            options=options,\n",
    "            parameters=parameters,\n",
    "            filter_update_function=filter_update_function,\n",
    "            return_parameter=return_parameter,\n",
    "            function_name=function_name,\n",
    "            description=description,\n",
    "            string_mapper=string_mapper,\n",
    "        )\n",
    "\n",
    "    def _create_kernel_function(\n",
    "        self,\n",
    "        search_type: SearchType,\n",
    "        options: SearchOptions | None = None,\n",
    "        parameters: list[KernelParameterMetadata] | None = None,\n",
    "        filter_update_function: DynamicFilterFunction | None = None,\n",
    "        return_parameter: KernelParameterMetadata | None = None,\n",
    "        function_name: str = DEFAULT_FUNCTION_NAME,\n",
    "        description: str = DEFAULT_DESCRIPTION,\n",
    "        string_mapper: Callable[[VectorSearchResult[TModel]], str] | None = None,\n",
    "    ) -> KernelFunction:\n",
    "        \"\"\"Create a kernel function from a search function.\"\"\"\n",
    "        update_func = filter_update_function or default_dynamic_filter_function\n",
    "\n",
    "        @kernel_function(name=function_name, description=description)\n",
    "        async def search_wrapper(**kwargs: Any) -> Sequence[str]:\n",
    "            query = kwargs.pop(\"query\", \"\")\n",
    "            try:\n",
    "                inner_options = create_options(self.options_class, deepcopy(options), **kwargs)\n",
    "            except ValidationError:\n",
    "                # this usually only happens when the kwargs are invalid, so blank options in this case.\n",
    "                inner_options = self.options_class()\n",
    "            inner_options.filter = update_func(filter=inner_options.filter, parameters=parameters, **kwargs)\n",
    "            match search_type:\n",
    "                case SearchType.VECTOR:\n",
    "                    try:\n",
    "                        results = await self.search(\n",
    "                            values=query,\n",
    "                            **inner_options.model_dump(exclude_defaults=True, exclude_none=True),\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        msg = f\"Exception in search function: {e}\"\n",
    "                        logger.error(msg)\n",
    "                        raise TextSearchException(msg) from e\n",
    "                case SearchType.KEYWORD_HYBRID:\n",
    "                    try:\n",
    "                        results = await self.hybrid_search(\n",
    "                            values=query,\n",
    "                            **inner_options.model_dump(exclude_defaults=True, exclude_none=True),\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        msg = f\"Exception in hybrid search function: {e}\"\n",
    "                        logger.error(msg)\n",
    "                        raise TextSearchException(msg) from e\n",
    "                case _:\n",
    "                    raise VectorStoreOperationNotSupportedException(\n",
    "                        f\"Search type '{search_type}' is not supported by this vector store: {self.__class__.__name__}\"\n",
    "                    )\n",
    "            if string_mapper:\n",
    "                return [string_mapper(result) async for result in results.results]\n",
    "            return [result.model_dump_json(exclude_none=True) async for result in results.results]\n",
    "\n",
    "        return KernelFunctionFromMethod(\n",
    "            method=search_wrapper,\n",
    "            parameters=DEFAULT_PARAMETER_METADATA if parameters is None else parameters,\n",
    "            return_parameter=return_parameter or DEFAULT_RETURN_PARAMETER_METADATA,\n",
    "        )\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class VectorStoreCollectionProtocol(Protocol):  # noqa: D101\n",
    "    collection_name: str\n",
    "    record_type: object\n",
    "    definition: VectorStoreCollectionDefinition\n",
    "    supported_key_types: ClassVar[set[str]]\n",
    "    supported_vector_types: ClassVar[set[str]]\n",
    "    embedding_generator: EmbeddingGeneratorBase | None = None\n",
    "\n",
    "    async def ensure_collection_exists(self, **kwargs: Any) -> bool:\n",
    "        \"\"\"Create the collection in the service if it does not exists.\n",
    "\n",
    "        First uses does_collection_exist to check if it exists, if it does returns False.\n",
    "        Otherwise, creates the collection and returns True.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the collection was created, False if it already exists.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def collection_exists(self, **kwargs: Any) -> bool:\n",
    "        \"\"\"Check if the collection exists.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the collection exists, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            Make sure the implementation of this function raises relevant exceptions with good descriptions.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def ensure_collection_deleted(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Delete the collection.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional arguments.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        key: Any = None,\n",
    "        keys: Sequence[Any] | None = None,\n",
    "        include_vectors: bool = False,\n",
    "        top: int | None = None,\n",
    "        skip: int | None = None,\n",
    "        order_by: OneOrMany[str] | dict[str, bool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> OptionalOneOrList[Any]:\n",
    "        \"\"\"Get a batch of records whose keys exist in the collection, i.e. keys that do not exist are ignored.\n",
    "\n",
    "        Args:\n",
    "            key: The key to get.\n",
    "            keys: The keys to get, if keys are provided, key is ignored.\n",
    "            include_vectors: Include the vectors in the response. Default is False.\n",
    "                Some vector stores do not support retrieving without vectors, even when set to false.\n",
    "                Some vector stores have specific parameters to control that behavior, when\n",
    "                that parameter is set, include_vectors is ignored.\n",
    "            top: The number of records to return.\n",
    "                Only used if keys are not provided.\n",
    "            skip: The number of records to skip.\n",
    "                Only used if keys are not provided.\n",
    "            order_by: The order by clause,\n",
    "                this can be a string, a list of strings or a dict,\n",
    "                when passing strings, they are assumed to be ascending.\n",
    "                Otherwise, use the value in the dict to set ascending (True) or descending (False).\n",
    "                example: {\"field_name\": True} or [\"field_name\", {\"field_name2\": False}].\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            The records, either a list of TModel or the container type.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during the get.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def upsert(\n",
    "        self,\n",
    "        records: OneOrMany[Any],\n",
    "        **kwargs: Any,\n",
    "    ) -> OneOrMany[Any]:\n",
    "        \"\"\"Upsert one or more records.\n",
    "\n",
    "        If the key of the record already exists, the existing record will be updated.\n",
    "        If the key does not exist, a new record will be created.\n",
    "\n",
    "        Args:\n",
    "            records: The records to upsert, can be a single record, a list of records, or a single container.\n",
    "                If a single record is passed, a single key is returned, instead of a list of keys.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            OneOrMany[Any]: The keys of the upserted records.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreModelSerializationException: If an error occurs during serialization.\n",
    "            VectorStoreOperationException: If an error occurs during upserting.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def delete(self, keys: OneOrMany[Any], **kwargs: Any) -> None:\n",
    "        \"\"\"Delete one or more records by key.\n",
    "\n",
    "        An exception will be raised at the end if any record does not exist.\n",
    "\n",
    "        Args:\n",
    "            keys: The key or keys to be deleted.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Raises:\n",
    "            VectorStoreOperationException: If an error occurs during deletion or a record does not exist.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class VectorSearchProtocol(VectorStoreCollectionProtocol, Protocol):\n",
    "    \"\"\"Protocol to check that a collection supports vector search.\"\"\"\n",
    "\n",
    "    supported_search_types: ClassVar[set[SearchType]]\n",
    "\n",
    "    async def search(\n",
    "        self,\n",
    "        values: Any = None,\n",
    "        *,\n",
    "        vector: Sequence[float | int] | None = None,\n",
    "        vector_property_name: str | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 3,\n",
    "        skip: int = 0,\n",
    "        include_total_count: bool = False,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult]:\n",
    "        \"\"\"Search the vector store for records that match the given value and filter.\n",
    "\n",
    "        Args:\n",
    "            values: The values to search for. These will be vectorized,\n",
    "                either by the store or using the provided generator.\n",
    "            vector: The vector to search for, if not provided, the values will be used to generate a vector.\n",
    "            vector_property_name: The name of the vector property to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Returns:\n",
    "            The search results.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    async def hybrid_search(\n",
    "        self,\n",
    "        values: Any,\n",
    "        *,\n",
    "        vector: list[float | int] | None = None,\n",
    "        vector_property_name: str | None = None,\n",
    "        additional_property_name: str | None = None,\n",
    "        filter: OptionalOneOrList[Callable | str] = None,\n",
    "        top: int = 3,\n",
    "        skip: int = 0,\n",
    "        include_total_count: bool = False,\n",
    "        include_vectors: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> KernelSearchResults[VectorSearchResult]:\n",
    "        \"\"\"Search the vector store for records that match the given values and filter using hybrid search.\n",
    "\n",
    "        Args:\n",
    "            values: The values to search for.\n",
    "            vector: The vector to search for, if not provided, the values will be used to generate a vector.\n",
    "            vector_property_name: The name of the vector field to use for the search.\n",
    "            additional_property_name: The name of the additional property field to use for the search.\n",
    "            filter: The filter to apply to the search.\n",
    "            top: The number of results to return.\n",
    "            skip: The number of results to skip.\n",
    "            include_total_count: Whether to include the total count of results.\n",
    "            include_vectors: Whether to include the vectors in the results.\n",
    "            kwargs: If options are not set, this is used to create them.\n",
    "                they are passed on to the inner search method.\n",
    "\n",
    "        Returns:\n",
    "            The search results.\n",
    "\n",
    "        Raises:\n",
    "            VectorSearchExecutionException: If an error occurs during the search.\n",
    "            VectorStoreModelDeserializationException: If an error occurs during deserialization.\n",
    "            VectorSearchOptionsException: If the search options are invalid.\n",
    "            VectorStoreOperationNotSupportedException: If the search type is not supported.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"DEFAULT_DESCRIPTION\",\n",
    "    \"DEFAULT_FUNCTION_NAME\",\n",
    "    \"DEFAULT_PARAMETER_METADATA\",\n",
    "    \"DEFAULT_RETURN_PARAMETER_METADATA\",\n",
    "    \"DISTANCE_FUNCTION_DIRECTION_HELPER\",\n",
    "    \"DistanceFunction\",\n",
    "    \"DynamicFilterFunction\",\n",
    "    \"FieldTypes\",\n",
    "    \"IndexKind\",\n",
    "    \"KernelSearchResults\",\n",
    "    \"SearchType\",\n",
    "    \"VectorSearch\",\n",
    "    \"VectorSearchProtocol\",\n",
    "    \"VectorSearchResult\",\n",
    "    \"VectorStore\",\n",
    "    \"VectorStoreCollection\",\n",
    "    \"VectorStoreCollectionDefinition\",\n",
    "    \"VectorStoreCollectionProtocol\",\n",
    "    \"VectorStoreField\",\n",
    "    \"create_options\",\n",
    "    \"default_dynamic_filter_function\",\n",
    "    \"vectorstoremodel\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da74fa33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel.data.vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Annotated\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreField, vectorstoremodel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@vectorstoremodel\u001b[39m(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSimpleModel\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple model to store some text with a ID.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'semantic_kernel.data.vector'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated\n",
    "from uuid import uuid4\n",
    "\n",
    "@vectorstoremodel(collection_name=\"simple-model\")\n",
    "@dataclass\n",
    "class SimpleModel:\n",
    "    \"\"\"Simple model to store some text with a ID.\"\"\"\n",
    "\n",
    "    text: Annotated[str, VectorStoreField(\"data\", is_full_text_indexed=True)]\n",
    "    id: Annotated[str, VectorStoreField(\"key\")] = field(default_factory=lambda: str(uuid4()))\n",
    "    embedding: Annotated[\n",
    "        list[float] | str | None, VectorStoreField(\"vector\", dimensions=1536, embedding_generator=embedding_gen)\n",
    "    ] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.embedding is None:\n",
    "            self.embedding = self.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7fefb6a",
   "metadata": {},
   "source": [
    "At its core, Semantic Memory is a set of data structures that allow you to store the meaning of text that come from different data sources, and optionally to store the source text too. These texts can be from the web, e-mail providers, chats, a database, or from your local directory, and are hooked up to the Semantic Kernel through data source connectors.\n",
    "\n",
    "The texts are embedded or compressed into a vector of floats representing mathematically the texts' contents and meaning. You can read more about embeddings [here](https://aka.ms/sk/embeddings).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7e7ca4",
   "metadata": {},
   "source": [
    "### Manually adding memories\n",
    "\n",
    "Let's create some initial memories \"About Me\". We can add memories to our `VolatileMemoryStore` by using `SaveInformationAsync`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d096504c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m records \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mSimpleModel\u001b[49m(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour budget for 2024 is $100,000\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     SimpleModel(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour savings from 2023 are $50,000\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m     SimpleModel(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour investments are $80,000\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleModel' is not defined"
     ]
    }
   ],
   "source": [
    "records = [\n",
    "    SimpleModel(text=\"Your budget for 2024 is $100,000\"),\n",
    "    SimpleModel(text=\"Your savings from 2023 are $50,000\"),\n",
    "    SimpleModel(text=\"Your investments are $80,000\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5338d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e6f2c669-0996-4cc5-8312-59e7d996a32b',\n",
       " '9042c465-115d-422d-b5f7-5ce35bd36455',\n",
       " 'ecc3ec62-b1b5-4a41-a761-63280d3a4329']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "collection = in_memory_store.get_collection(record_type=SimpleModel)\n",
    "await collection.ensure_collection_exists()\n",
    "# Add records to the collection\n",
    "await collection.upsert(records)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2calf857",
   "metadata": {},
   "source": [
    "Let's try searching the memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.data.vector import VectorSearchProtocol\n",
    "\n",
    "\n",
    "async def search_memory_examples(collection: VectorSearchProtocol, questions: list[str]) -> None:\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        results = await collection.search(question, top=1)\n",
    "        async for result in results.results:\n",
    "            print(f\"Answer: {result.record.text}\")\n",
    "            print(f\"Score: {result.score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe185991",
   "metadata": {},
   "source": [
    "The default distance metric for the InMemoryCollection is `cosine`, this means that the closer the vectors are, the more similar they are. The `search` method will return the top `3` results by default, but you can change this by passing in the `top` parameter, this is set to `1` here to get only the most relevant result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24764c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is my budget for 2024?\n",
      "Answer: Your budget for 2024 is $100,000\n",
      "Score: 0.20661429378068685\n",
      "\n",
      "Question: What are my savings from 2023?\n",
      "Answer: Your savings from 2023 are $50,000\n",
      "Score: 0.2028375831967465\n",
      "\n",
      "Question: What are my investments?\n",
      "Answer: Your investments are $80,000\n",
      "Score: 0.33724588631042385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await search_memory_examples(\n",
    "    collection,\n",
    "    questions=[\n",
    "        \"What is my budget for 2024?\",\n",
    "        \"What are my savings from 2023?\",\n",
    "        \"What are my investments?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133d9d6",
   "metadata": {},
   "source": [
    "Next, we will add a search function to our kernel that will allow us to search the memory store for relevant information. This function will use the `search` method of the `InMemoryStore` to find the most relevant memories based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1fdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = kernel.add_function(\n",
    "    plugin_name=\"memory\",\n",
    "    function=collection.create_search_function(\n",
    "        function_name=\"recall\",\n",
    "        description=\"Searches the memory for relevant information based on the input query.\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbaf80",
   "metadata": {},
   "source": [
    "Then we will create a prompt that will use the search function to find relevant information in the memory store and return it as part of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelFunction\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "\n",
    "\n",
    "async def setup_chat_with_memory(\n",
    "    kernel: Kernel,\n",
    "    service_id: str,\n",
    ") -> KernelFunction:\n",
    "    prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{recall 'budget by year'}} What is my budget for 2024?\n",
    "    - {{recall 'savings from previous year'}} What are my savings from 2023?\n",
    "    - {{recall 'investments'}} What are my investments?\n",
    "\n",
    "    {{$request}}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    prompt_template_config = PromptTemplateConfig(\n",
    "        template=prompt,\n",
    "        execution_settings={\n",
    "            service_id: kernel.get_service(service_id).get_prompt_execution_settings_class()(service_id=service_id)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return kernel.add_function(\n",
    "        function_name=\"chat_with_memory\",\n",
    "        plugin_name=\"chat\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645b55a1",
   "metadata": {},
   "source": [
    "Now that we've included our memories, let's chat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3875a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a chat (with memory!)\n",
      "Begin chatting (type 'exit' to exit):\n",
      "\n",
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try asking a question about your finances (i.e. \"talk to me about my finances\").\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func = await setup_chat_with_memory(kernel, chat_service_id)\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try asking a question about your finances (i.e. \\\"talk to me about my finances\\\").\"\n",
    ")\n",
    "\n",
    "\n",
    "async def chat(user_input: str):\n",
    "    print(f\"User: {user_input}\")\n",
    "    answer = await kernel.invoke(chat_func, request=user_input)\n",
    "    print(f\"ChatBot:> {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b55f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is my budget for 2024?\n",
      "ChatBot:> Your budget for 2024 is $100,000.\n"
     ]
    }
   ],
   "source": [
    "await chat(\"What is my budget for 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243f9eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: talk to me about my finances\n",
      "ChatBot:> Based on our previous conversations, your investments amount to $80,000. Your financial overview includes a budget for 2024 of $100,000 and savings from 2023 totaling $50,000. If you'd like, I can help you analyze your financial goals, suggest investment strategies, or assist with budgeting. Let me know how you'd like to proceed!\n"
     ]
    }
   ],
   "source": [
    "await chat(\"talk to me about my finances\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a51542b",
   "metadata": {},
   "source": [
    "### Adding documents to your memory\n",
    "\n",
    "Many times in your applications you'll want to bring in external documents into your memory. Let's see how we can do this using our VolatileMemoryStore.\n",
    "\n",
    "Let's first get some data using some of the links in the Semantic Kernel repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170e7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorstoremodel(collection_name=\"github-files\")\n",
    "@dataclass\n",
    "class GitHubFile:\n",
    "    \"\"\"\n",
    "    Model to store GitHub file URLs and their descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    url: Annotated[str, VectorStoreField(\"data\", is_full_text_indexed=True)]\n",
    "    text: Annotated[str, VectorStoreField(\"data\", is_full_text_indexed=True)]\n",
    "    key: Annotated[str, VectorStoreField(\"key\")] = field(default_factory=lambda: str(uuid4()))\n",
    "    embedding: Annotated[\n",
    "        list[float] | str | None, VectorStoreField(\"vector\", dimensions=1536, embedding_generator=embedding_gen)\n",
    "    ] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.embedding is None:\n",
    "            self.embedding = f\"{self.url} {self.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d971eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['81585815-6be9-4b45-b608-af1f9918567f',\n",
       " '83d5e6ba-a6f9-4d05-aa27-0b5470c1aa83',\n",
       " 'e015e196-ddb3-4157-9291-dcef201d5308']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_files = []\n",
    "github_files.append(\n",
    "    GitHubFile(\n",
    "        url=\"https://github.com/microsoft/semantic-kernel/blob/main/README.md\",\n",
    "        text=\"README: Installation, getting started, and how to contribute\",\n",
    "    )\n",
    ")\n",
    "github_files.append(\n",
    "    GitHubFile(\n",
    "        url=\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/02-running-prompts-from-file.ipynb\",\n",
    "        text=\"Jupyter notebook describing how to pass prompts from a file to a semantic plugin or function\",\n",
    "    )\n",
    ")\n",
    "github_files.append(\n",
    "    GitHubFile(\n",
    "        url=\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/00-getting-started.ipynb\",\n",
    "        text=\"Jupyter notebook describing how to get started with Semantic Kernel\",\n",
    "    )\n",
    ")\n",
    "\n",
    "github_memory = in_memory_store.get_collection(record_type=GitHubFile)\n",
    "await github_memory.ensure_collection_exists()\n",
    "\n",
    "# Add records to the collection\n",
    "await github_memory.upsert(github_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "143911c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Query: I love Jupyter notebooks, how should I get started?\n",
      "\n",
      "Result e015e196-ddb3-4157-9291-dcef201d5308:\n",
      "  URL        : https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/00-getting-started.ipynb\n",
      "  Text       : Jupyter notebook describing how to get started with Semantic Kernel\n",
      "  Relevance  : 0.38310321217805676\n",
      "\n",
      "Result 83d5e6ba-a6f9-4d05-aa27-0b5470c1aa83:\n",
      "  URL        : https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/02-running-prompts-from-file.ipynb\n",
      "  Text       : Jupyter notebook describing how to pass prompts from a file to a semantic plugin or function\n",
      "  Relevance  : 0.5391694801842719\n",
      "\n",
      "Result 81585815-6be9-4b45-b608-af1f9918567f:\n",
      "  URL        : https://github.com/microsoft/semantic-kernel/blob/main/README.md\n",
      "  Text       : README: Installation, getting started, and how to contribute\n",
      "  Relevance  : 0.6495028830870797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask = \"I love Jupyter notebooks, how should I get started?\"\n",
    "print(\"===========================\\n\" + \"Query: \" + ask + \"\\n\")\n",
    "\n",
    "memories = await github_memory.search(ask, top=5)\n",
    "\n",
    "async for result in memories.results:\n",
    "    memory = result.record\n",
    "    print(f\"Result {memory.key}:\")\n",
    "    print(f\"  URL        : {memory.url}\")\n",
    "    print(f\"  Text       : {memory.text}\")\n",
    "    print(f\"  Relevance  : {result.score}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59294dac",
   "metadata": {},
   "source": [
    "Now you might be wondering what happens if you have so much data that it doesn't fit into your RAM? That's where you want to make use of an external Vector Database made specifically for storing and retrieving embeddings. Fortunately, semantic kernel makes this easy thanks to an extensive list of available connectors. In the following section, we will connect to an existing Azure AI Search service that we will use as an external Vector Database to store and retrieve embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fd381",
   "metadata": {},
   "source": [
    "_Please note you will need an AzureAI Search api_key or token credential and endpoint for the following example to work properly._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdfa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['81585815-6be9-4b45-b608-af1f9918567f',\n",
       " '83d5e6ba-a6f9-4d05-aa27-0b5470c1aa83',\n",
       " 'e015e196-ddb3-4157-9291-dcef201d5308']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel.connectors.azure_ai_search import AzureAISearchCollection\n",
    "\n",
    "azs_memory = AzureAISearchCollection(record_type=GitHubFile)\n",
    "# We explicitly delete the collection if it exists to ensure a clean state\n",
    "await azs_memory.ensure_collection_deleted()\n",
    "await azs_memory.ensure_collection_exists()\n",
    "# Add records to the collection\n",
    "await azs_memory.upsert(github_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9e83b",
   "metadata": {},
   "source": [
    "The implementation of Semantic Kernel allows to easily swap memory store for another. Here, we will re-use the functions we initially created for `InMemoryStore` with our new external Vector Store leveraging Azure AI Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbe830",
   "metadata": {},
   "source": [
    "Let's now try to query from Azure AI Search! Note that the `score` might be different because the AzureAISearchCollection uses a different default distance metric then InMemoryCollection, if you specify the `distance_function` in the model, you can get the same results as with the InMemoryCollection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a09d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Semantic Kernel?\n",
      "Answer: README: Installation, getting started, and how to contribute\n",
      "Score: 0.72694075\n",
      "\n",
      "Question: How do I get started on it with notebooks?\n",
      "Answer: Jupyter notebook describing how to get started with Semantic Kernel\n",
      "Score: 0.68709856\n",
      "\n",
      "Question: Where can I find more info on prompts?\n",
      "Answer: Jupyter notebook describing how to pass prompts from a file to a semantic plugin or function\n",
      "Score: 0.6572231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await search_memory_examples(\n",
    "    azs_memory,\n",
    "    questions=[\n",
    "        \"What is Semantic Kernel?\",\n",
    "        \"How do I get started on it with notebooks?\",\n",
    "        \"Where can I find more info on prompts?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0800fc",
   "metadata": {},
   "source": [
    "Make sure to cleanup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cd429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await azs_memory.ensure_collection_deleted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33dcdc",
   "metadata": {},
   "source": [
    "We have laid the foundation which will allow us to store an arbitrary amount of data in an external Vector Store above and beyond what could fit in memory at the expense of a little more latency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
